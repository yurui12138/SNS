# SNSæ–¹æ³•å®ç°å¯¹æ¯”åˆ†ææŠ¥å‘Š

## æ‰§è¡Œæ¦‚è¦

æœ¬æŠ¥å‘Šè¯¦ç»†åˆ†æäº†SNS (Self-Nonself) æ–¹æ³•è¯´æ˜æ–‡æ¡£ä¸å½“å‰ä»£ç å®ç°çš„å¯¹é½æƒ…å†µ,è¯†åˆ«ç¼ºå¤±åŠŸèƒ½å¹¶æå‡ºæ”¹è¿›æ–¹æ¡ˆã€‚

**æ—¥æœŸ**: 2025-12-15  
**åˆ†æèŒƒå›´**: Phase 1-4 å®Œæ•´æµç¨‹

---

## 1. æ€»ä½“å¯¹é½æƒ…å†µ

### âœ… å·²å®ç°çš„æ ¸å¿ƒåŠŸèƒ½

| æ–¹æ³•ç»„ä»¶ | å®ç°çŠ¶æ€ | æ–‡ä»¶ä½ç½® |
|---------|---------|---------|
| Phase 1: Multi-view Baseline | âœ… åŸºæœ¬å®ç° | `phase1_multiview_baseline.py` |
| Phase 2: Stress Testing | âœ… åŸºæœ¬å®ç° | `phase2_stress_test.py` |
| Phase 3: Evolution Planning | âœ… åŸºæœ¬å®ç° | `phase3_evolution.py` |
| Phase 4: Guidance Generation | âœ… åŸºæœ¬å®ç° | `phase4_guidance.py` |
| æ ¸å¿ƒæ•°æ®ç»“æ„ | âœ… å®Œæ•´å®ç° | `dataclass_v2.py` |

### âš ï¸ éœ€è¦æ”¹è¿›çš„å…³é”®åŠŸèƒ½

## 2. Phase 1: Multi-view Baseline Construction - è¯¦ç»†åˆ†æ

### 2.1 æ–¹æ³•è¯´æ˜è¦æ±‚

**ç›®æ ‡**: å°†ç›®æ ‡é¢†åŸŸæ—¢æœ‰ç»¼è¿°å½¢æˆçš„"è‡ªæˆ‘ï¼ˆSelfï¼‰è®¤çŸ¥ç»“æ„"æ˜¾å¼å»ºæ¨¡ä¸ºå¤šè§†è§’ç»„ç»‡åŸºçº¿

**å…³é”®æ­¥éª¤**:
1. ç»¼è¿°æ£€ç´¢ä¸ç­›é€‰ (æŒ‰review/survey/tutorialå…³é”®è¯)
2. æŠ½å–taxonomyç»“æ„ (ä»ç›®å½•/ç« èŠ‚/å¼•è¨€)
3. è§†è§’æ ‡ç­¾è¯†åˆ« (ä»å—æ§æšä¸¾é›†é€‰æ‹©facet)
4. èŠ‚ç‚¹å®šä¹‰æ„å»º (definition, inclusion_criteria â‰¥3, exclusion_criteria â‰¥2, keywords, evidence_spans)
5. å¤šè§†è§’é—¸é—¨ä¸è¡¥è§†è§’ (unique(facet) < 2æ—¶è§¦å‘è¡¥è§†è§’ç­–ç•¥)
6. æƒé‡è®¡ç®—: w_i âˆ Recency Ã— Quality Ã— Coverage

### 2.2 å½“å‰ä»£ç å®ç°

#### âœ… å·²å®ç°åŠŸèƒ½

1. **ReviewRetriever** (lines 32-189):
   - âœ… åŸºäºå…³é”®è¯æ£€ç´¢ç»¼è¿° (`survey`, `review`, `overview` ç­‰)
   - âœ… å¯å‘å¼ç­›é€‰ (titleå…³é”®è¯, æ‘˜è¦é•¿åº¦ >120 words)
   - âœ… è´¨é‡è¯„åˆ† `0.4*recency + 0.4*citation + 0.2*relevance`
   - âœ… å¹´ä»½æå– (ä»snippets)

2. **TaxonomyViewExtractor** (lines 191-325):
   - âœ… ä½¿ç”¨LLMæå–taxonomy tree (JSON schema, temperature=0)
   - âœ… è§£æfacet_label (FacetLabelæšä¸¾)
   - âœ… é€’å½’æ„å»ºTaxonomyTree

3. **NodeDefinitionBuilder** (lines 327-458):
   - âœ… ä¸ºæ¯ä¸ªèŠ‚ç‚¹æ„å»ºå®šä¹‰
   - âœ… åŒ…å« definition, inclusion_criteria, exclusion_criteria, canonical_keywords, boundary_statements
   - âœ… ç»‘å®ševidence_spansåˆ°æºæ–‡æœ¬

4. **MultiViewBaselineBuilder** (lines 460-595):
   - âœ… æƒé‡è®¡ç®— `recency * quality * coverage`
   - âœ… æƒé‡å½’ä¸€åŒ– (åœ¨MultiViewBaseline.__post_init__)
   - âœ… è´¨é‡é—¸é—¨æ£€æŸ¥:
     - âœ… unique facets < 2 æ—¶å‘å‡ºè­¦å‘Š
     - âœ… dominant facet > 60% æ—¶å‘å‡ºè­¦å‘Š

#### âŒ ç¼ºå¤±åŠŸèƒ½

**CRITICAL**: **è¡¥è§†è§’ç­–ç•¥æœªå®ç°**

æ–¹æ³•è¯´æ˜è¦æ±‚:
> è‹¥ `unique(facet) < 2` æˆ– facet åˆ†å¸ƒé€€åŒ–,è§¦å‘è¡¥è§†è§’ç­–ç•¥:  
> ä»å‰æ²¿è®ºæ–‡é›†åˆ(æˆ–ç»¼è¿°è¯­æ–™)è¯±å¯¼æ„å»º `T_extra`(ä»¥èšç±»å¾—åˆ°çš„ä¸»é¢˜ç°‡å‘½åå½¢æˆæ ‘),ä½¿å›¾é›†å…·å¤‡è‡³å°‘ä¸¤ç±»è§†è§’ã€‚

**å½“å‰ä»£ç **:
```python
# phase1_multiview_baseline.py, lines 515-552
def _check_baseline_quality(self, baseline: MultiViewBaseline) -> None:
    # ...
    if num_unique_facets < 2:
        logger.warning("âš ï¸ QUALITY GATE WARNING: Only {num_unique_facets} unique facets")
        logger.warning("   Consider retrieving more diverse reviews or inducing additional views")
    # âŒ æ²¡æœ‰å®é™…çš„è¡¥è§†è§’å®ç°ï¼åªæ˜¯è­¦å‘Š
```

**å½±å“**: å½“ç»¼è¿°æ•°æ®ä¸è¶³æˆ–è´¨é‡ä½æ—¶,ç³»ç»Ÿæ— æ³•è‡ªåŠ¨è¡¥æ•‘,å¯¼è‡´baselineè´¨é‡ä¸è¾¾æ ‡ã€‚

#### âš ï¸ éœ€è¦å¢å¼ºçš„åŠŸèƒ½

1. **èŠ‚ç‚¹å®šä¹‰è´¨é‡**:
   - æ–¹æ³•è¦æ±‚: `inclusion_criteria â‰¥ 3`, `exclusion_criteria â‰¥ 2`
   - å½“å‰å®ç°: æ²¡æœ‰ç¡¬æ€§éªŒè¯,å¯èƒ½ç”Ÿæˆä¸è¶³3/2çš„æ ‡å‡†
   - **å»ºè®®**: æ·»åŠ schemaéªŒè¯å’ŒLLM promptçº¦æŸ

2. **Evidence Spansè´¨é‡**:
   - æ–¹æ³•å¼ºè°ƒ: "æ‰€æœ‰èŠ‚ç‚¹å®šä¹‰ã€é€‚é…åŸå› ä¸æ–°å¢ç»“æ„å‡ç»‘å®šåŸæ–‡spans"
   - å½“å‰å®ç°: ä¾èµ–LLMç”Ÿæˆ,æ²¡æœ‰éªŒè¯char_start/char_endå‡†ç¡®æ€§
   - **å»ºè®®**: æ·»åŠ evidenceå®Œæ•´æ€§æ£€æŸ¥

### 2.3 æ”¹è¿›å»ºè®®

#### **æ”¹è¿›1: å®ç°è¡¥è§†è§’ç­–ç•¥ (CompensatoryViewInducer)**

```python
class CompensatoryViewInducer:
    """
    å½“baselineè´¨é‡ä¸è¶³æ—¶,ä»å‰æ²¿è®ºæ–‡è¯±å¯¼è¡¥è§†è§’ã€‚
    
    ç­–ç•¥:
    1. ä½¿ç”¨HDBSCANå¯¹å‰æ²¿è®ºæ–‡èšç±» (åŸºäºtitle+abstract embeddings)
    2. ä¸ºæ¯ä¸ªç°‡ç”Ÿæˆä¸»é¢˜æ ‡ç­¾ (ä½¿ç”¨LLM)
    3. æ„å»ºinduced taxonomy tree (ä»¥ç°‡æ ‡ç­¾ä¸ºå¶èŠ‚ç‚¹)
    4. åˆ†é…æ–°çš„facet_label (é¿å…ä¸ç°æœ‰facetå†²çª)
    """
    
    def induce_compensatory_view(
        self,
        baseline: MultiViewBaseline,
        papers: List[Information],  # å‰æ²¿è®ºæ–‡
        min_facet_count: int = 2
    ) -> Optional[TaxonomyView]:
        """è¯±å¯¼è¡¥è§†è§’ä»¥æ»¡è¶³å¤šæ ·æ€§è¦æ±‚"""
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è¡¥è§†è§’
        facet_counts = Counter([v.facet_label for v in baseline.views])
        if len(facet_counts) >= min_facet_count:
            return None  # è´¨é‡è¾¾æ ‡,æ— éœ€è¡¥æ•‘
        
        # 1. å¯¹è®ºæ–‡èšç±»
        clusters = self._cluster_papers(papers)
        
        # 2. ä¸ºæ¯ä¸ªç°‡ç”Ÿæˆæ ‡ç­¾
        cluster_labels = self._generate_cluster_labels(clusters)
        
        # 3. æ„å»ºinduced tree
        induced_tree = self._build_induced_tree(cluster_labels)
        
        # 4. åˆ›å»ºæ–°è§†è§’
        compensatory_view = TaxonomyView(
            view_id=f"T_induced_{len(baseline.views)+1}",
            review_id="INDUCED_FROM_PAPERS",
            review_title="Induced View from Research Papers",
            facet_label=self._select_unique_facet(baseline),  # é€‰æ‹©æœªä½¿ç”¨çš„facet
            facet_rationale="Compensatory view induced from paper clustering",
            tree=induced_tree,
            node_definitions={},
            weight=0.5,  # ç•¥ä½äºæ­£å¸¸ç»¼è¿°
            evidence=[]
        )
        
        return compensatory_view
```

**é›†æˆä½ç½®**: `Phase1Pipeline.run()` çš„Step 4ä¹‹å

#### **æ”¹è¿›2: å¢å¼ºèŠ‚ç‚¹å®šä¹‰éªŒè¯**

```python
def _validate_node_definition(self, node_def: NodeDefinition) -> bool:
    """éªŒè¯èŠ‚ç‚¹å®šä¹‰è´¨é‡"""
    errors = []
    
    # è§„åˆ™1: inclusion_criteria â‰¥ 3
    if len(node_def.inclusion_criteria) < 3:
        errors.append(f"inclusion_criteria count={len(node_def.inclusion_criteria)} < 3")
    
    # è§„åˆ™2: exclusion_criteria â‰¥ 2
    if len(node_def.exclusion_criteria) < 2:
        errors.append(f"exclusion_criteria count={len(node_def.exclusion_criteria)} < 2")
    
    # è§„åˆ™3: å¿…é¡»æœ‰evidence_spans
    if not node_def.evidence_spans:
        errors.append("Missing evidence_spans")
    
    if errors:
        logger.warning(f"Node {node_def.node_path} validation failed: {errors}")
        return False
    
    return True
```

---

## 3. Phase 2: Multi-view Stress Test - è¯¦ç»†åˆ†æ

### 3.1 æ–¹æ³•è¯´æ˜è¦æ±‚

**ç›®æ ‡**: å°†æœ€æ–°éç»¼è¿°è®ºæ–‡è§†ä¸ºæ½œåœ¨"éæˆ‘ï¼ˆNonselfï¼‰"è¾“å…¥,é€šè¿‡è·¨è§†è§’é€‚é…æµ‹è¯•è¯†åˆ«ç»“æ„å‹åŠ›

**å…³é”®æ­¥éª¤**:
1. **è®ºæ–‡ä¸»å¼ æŠ½å–**: æŠ½å–problem, core_idea, mechanism, training, evaluation, novelty_bullets (æ°å¥½3æ¡)
2. **å€™é€‰å¶èŠ‚ç‚¹å¬å›**: Embeddingç›¸ä¼¼åº¦ + å…³é”®è¯åŒ¹é…, Top-Kå€™é€‰
3. **é€‚é…æ‰“åˆ† (Tri-factor)**:
   - **Coverage**: `0.7 Ã— cos(emb) + 0.3 Ã— Jaccard(keywords)`
   - **Conflict**: `max_{hâˆˆExclusion} P_NLI(contradiction|claim,h)` - éœ€è¦NLIæ¨¡å‹
   - **Residual**: `1 - max_{bâˆˆNoveltyBullets} cos(emb(b), leaf_vector)`
4. **æ ‡ç­¾åˆ¤å®š**:
   ```
   if Coverage < 0.45 or Conflict > 0.55: UNFITTABLE
   elif Residual > 0.45: FORCE_FIT
   else: FIT
   ```
5. **è¯æ®è¾“å‡º**: lost_novelty, conflict_evidence, spans

### 3.2 å½“å‰ä»£ç å®ç°

#### âœ… å·²å®ç°åŠŸèƒ½

1. **PaperClaimExtractor** (lines 36-138):
   - âœ… ä½¿ç”¨LLMæå–ç»“æ„åŒ–claims (JSON schema)
   - âœ… å¼ºåˆ¶novelty_bullets = 3 (padding/trimming)
   - âœ… ç»‘å®ševidenceåˆ°PaperClaim

2. **EmbeddingBasedRetriever** (lines 140-224):
   - âœ… Top-Kå€™é€‰å¬å› (åŸºäºç›¸ä¼¼åº¦æ’åº)
   - âš ï¸ **Placeholderå®ç°**: ä½¿ç”¨ç®€å•keyword overlap,ä¸æ˜¯çœŸå®embeddings

3. **FitTester** (lines 226-435):
   - âœ… Coverageè®¡ç®— `0.7*semantic + 0.3*lexical`
   - âš ï¸ **Placeholder Conflict**: ä½¿ç”¨keyword overlap,ä¸æ˜¯NLIæ¨¡å‹
   - âœ… Residualè®¡ç®— `1 - max(novelty_sim)`
   - âœ… é˜ˆå€¼åˆ¤å®šè§„åˆ™ (0.45, 0.55, 0.45)
   - âœ… æå–lost_noveltyå’Œconflict_evidence

4. **MultiViewStressTester** (lines 437-552):
   - âœ… å¯¹æ‰€æœ‰è§†è§’æµ‹è¯•
   - âœ… åŠ æƒstress_scoreå’Œunfittable_score

#### âŒ ç¼ºå¤±åŠŸèƒ½

**CRITICAL 1**: **NLIå†²çªæ£€æµ‹æœªå®ç°**

æ–¹æ³•è¯´æ˜æ˜ç¡®è¦æ±‚:
> **Conflict**: `max_{h âˆˆ Exclusion} P_NLI(contradiction | claim, h)`
> - ä½¿ç”¨DeBERTa-MNLIæ¨¡å‹æ£€æµ‹entailment

**å½“å‰ä»£ç **:
```python
# phase2_stress_test.py, lines 335-347
def _keyword_conflict_score(self, claim: str, exclusion: str) -> float:
    """
    Placeholder conflict detection based on keywords.
    
    In production: Replace with NLI model prediction.  # âŒ è¯´æ˜è¿™æ˜¯TODO
    """
    # ç®€å•keyword overlap
```

**å½±å“**: Conflictåˆ†æ•°ä¸å‡†ç¡®,å¯¼è‡´FIT/UNFITTABLEåˆ¤å®šå¯èƒ½é”™è¯¯ã€‚

**CRITICAL 2**: **çœŸå®Embeddingæ¨¡å‹æœªé›†æˆ**

**å½“å‰ä»£ç **:
```python
# phase2_stress_test.py, lines 146-149
def __init__(self, embedding_model_name: str = "dummy"):
    self.model_name = embedding_model_name
    # In production, load actual model:
    # from sentence_transformers import SentenceTransformer
    # self.model = SentenceTransformer('allenai/specter2')  # âŒ æœªå®é™…åŠ è½½
```

**å½±å“**: 
- Coverageä¸­çš„semanticåˆ†æ•°ä¸å‡†ç¡®
- Residualä¸­çš„noveltyç›¸ä¼¼åº¦ä¸å‡†ç¡®
- å€™é€‰å¬å›è´¨é‡å·®

#### âš ï¸ éœ€è¦å¢å¼ºçš„åŠŸèƒ½

1. **Evidence Spanç»‘å®šè´¨é‡**:
   - lost_noveltyå’Œconflict_evidenceéƒ½æœ‰evidenceå­—æ®µ
   - ä½†å½“å‰å®ç°å¯èƒ½ä¸¢å¤±ç²¾ç¡®çš„char_start/end
   
2. **é˜ˆå€¼å¯æ ¡å‡†**:
   - æ–¹æ³•è¯´æ˜: "ç”¨å°è§„æ¨¡æ ‡æ³¨æˆ–æ•æ„Ÿæ€§åˆ†æç¡®å®šé˜ˆå€¼"
   - å½“å‰: ç¡¬ç¼–ç  (0.45, 0.55, 0.45)
   - **å»ºè®®**: æ·»åŠ thresholdé…ç½®æ¥å£

### 3.3 æ”¹è¿›å»ºè®®

#### **æ”¹è¿›1: é›†æˆNLIå†²çªæ£€æµ‹**

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

class NLIConflictDetector:
    """
    ä½¿ç”¨DeBERTa-MNLIæ£€æµ‹claimä¸exclusionçš„å†²çªã€‚
    """
    
    def __init__(self, model_name: str = "microsoft/deberta-v3-base-mnli"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
        self.model.eval()
    
    def detect_conflict(self, claim: str, exclusion: str) -> float:
        """
        è®¡ç®—P_NLI(contradiction | claim, exclusion)
        
        Returns:
            contradictionæ¦‚ç‡ [0.0, 1.0]
        """
        inputs = self.tokenizer(
            claim, 
            exclusion,
            return_tensors="pt",
            truncation=True,
            max_length=512
        )
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            probs = torch.softmax(outputs.logits, dim=-1)
        
        # DeBERTa-MNLIè¾“å‡º: [entailment, neutral, contradiction]
        contradiction_prob = probs[0, 2].item()
        
        return contradiction_prob
```

**é›†æˆåˆ°FitTester**:
```python
class FitTester:
    def __init__(self, retriever, nli_detector: Optional[NLIConflictDetector] = None):
        self.retriever = retriever
        self.nli_detector = nli_detector  # æ–°å¢
    
    def _calculate_conflict(self, claims: PaperClaims, node_def: NodeDefinition) -> float:
        if self.nli_detector:
            # ä½¿ç”¨çœŸå®NLIæ¨¡å‹
            max_conflict = 0.0
            for claim in all_claims:
                for exclusion in all_exclusions:
                    conflict_score = self.nli_detector.detect_conflict(claim, exclusion)
                    max_conflict = max(max_conflict, conflict_score)
            return max_conflict
        else:
            # Fallback to keyword-based
            return self._keyword_conflict_score(...)
```

#### **æ”¹è¿›2: é›†æˆSPECTER2 Embeddings**

```python
from sentence_transformers import SentenceTransformer
import numpy as np

class ScientificEmbedder:
    """
    ä½¿ç”¨SPECTER2æˆ–SciNCLç”Ÿæˆç§‘å­¦è®ºæ–‡embeddingsã€‚
    """
    
    def __init__(self, model_name: str = "allenai/specter2"):
        self.model = SentenceTransformer(model_name)
        self.cache = {}  # ç¼“å­˜embeddings
    
    def encode(self, text: str) -> np.ndarray:
        """ç”Ÿæˆembedding"""
        if text in self.cache:
            return self.cache[text]
        
        emb = self.model.encode(text, show_progress_bar=False)
        self.cache[text] = emb
        return emb
    
    def cosine_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        emb1 = self.encode(text1)
        emb2 = self.encode(text2)
        
        return float(np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2)))
```

**æ›´æ–°EmbeddingBasedRetriever**:
```python
class EmbeddingBasedRetriever:
    def __init__(self, embedding_model_name: str = "allenai/specter2"):
        self.embedder = ScientificEmbedder(embedding_model_name)
    
    def _compute_similarity(self, text1: str, text2: str) -> float:
        return self.embedder.cosine_similarity(text1, text2)
```

---

## 4. Phase 3: Stress Clustering & Evolution - è¯¦ç»†åˆ†æ

### 4.1 æ–¹æ³•è¯´æ˜è¦æ±‚

**ç›®æ ‡**: é€šè¿‡è·¨è§†è§’é€‚é…æµ‹è¯•ã€ç»“æ„å‹åŠ›èšåˆä¸æœ€å°å¿…è¦ç»“æ„æ›´æ–°,ç”Ÿæˆå¯å®¡è®¡çš„è®¤çŸ¥å¢é‡

**å…³é”®æ­¥éª¤**:
1. **å‹åŠ›è®ºæ–‡ç­›é€‰**: stress_score > threshold (å¦‚0.3)
2. **å¤±è´¥ç­¾åæ„å»º**: facet + best_leaf_path + lost_novelty + å…³é”®æœ¯è¯­
3. **å‹åŠ›ç°‡èšç±»**: ä½¿ç”¨HDBSCAN (æ— éœ€æŒ‡å®šK)
4. **è·¨è§†è§’ä¸€è‡´æ€§åˆ¤å®š**: è®¡ç®— U(C), S(C), Ï_i(C)
   - `STRONG_SHIFT`: U(C) > 0.55 ä¸” â‰¥2ä¸ªé«˜æƒé‡è§†è§’å¤±è´¥
   - `FACET_DEPENDENT`: æ··åˆå¤±è´¥/é€‚é…
   - `STABLE`: å¤šæ•°é«˜æƒé‡è§†è§’é€‚é…
5. **å€™é€‰ç»“æ„æ›´æ–°ç”Ÿæˆ**: ADD/SPLIT/RENAME,ç»‘å®šè¯æ®å¡
6. **æœ€å°å¿…è¦æ›´æ–°é€‰æ‹©**: `Objective = FitGain - Î»Â·EditCost` (Î»=0.8)
7. **ä¸»è½´/è¾…è½´ç»„ç»‡æ–¹æ¡ˆç¡®å®š (Evolution-first)**:
   - **å…ˆé‡æ„å†é€‰æ‹©**: å¯¹å€™é€‰è§†è§’æ‰§è¡Œæœ€å°å¿…è¦é‡æ„å¾—åˆ°T_i'
   - è®¡ç®— `Score_i = Î±Â·FitGain + Î²Â·Stress + Î³Â·Coverage âˆ’ Î»Â·EditCost`
   - é€‰æ‹©ä¸¤ç§æ¨¡å¼ä¹‹ä¸€:
     - **Delta-first**: é‡æ„åçš„é«˜å‹åŠ›è§†è§’ä½œä¸ºä¸»è½´
     - **Anchor+Delta**: è¦†ç›–ç¨³å®šè§†è§’ä½œä¸ºé”šå®šä¸»è½´,é‡æ„è§†è§’ä½œä¸ºè´¯ç©¿è¾…è½´

### 4.2 å½“å‰ä»£ç å®ç°

#### âœ… å·²å®ç°åŠŸèƒ½

1. **StressClusterer** (lines 48-334):
   - âœ… ç­›é€‰ stress_score > 0.3
   - âœ… æ„å»ºfailure_signature (facet + leaf_path + lost_novelty)
   - âœ… HDBSCANèšç±» (with fallback)
   - âœ… è®¡ç®—view_failure_rates
   - âœ… ClusterTypeåˆ¤å®š (STRONG_SHIFT, FACET_DEPENDENT, STABLE)

2. **EvolutionPlanner** (lines 336-624):
   - âœ… ä¸ºæ¯ä¸ªclusteræè®®ADD_NODEæ“ä½œ
   - âœ… ä½¿ç”¨LLMç”ŸæˆNewNodeProposal
   - âœ… è®¡ç®—fit_gain (simplified as 0.5*cluster_size)
   - âœ… Greedy selection: `objective = fit_gain - lambda*edit_cost`
   - âœ… **compute_all_views_reconstruction()** (lines 419-512):
     - âœ… å¯¹æ‰€æœ‰è§†è§’è®¡ç®—reconstruction metrics
     - âœ… Combined score: `0.4*FitGain + 0.3*StressRed + 0.2*Coverage - 0.1*EditCost`
     - âœ… æ’åºè¾“å‡ºViewReconstructionScoreåˆ—è¡¨

3. **ViewReconstructionScore** (dataclass_v2.py, lines 720-778):
   - âœ… å®šä¹‰å®Œæ•´,åŒ…å« fit_gain, stress_reduction, coverage, edit_cost, combined_score
   - âœ… è‡ªåŠ¨è®¡ç®—combined_score (åœ¨__init__)

#### âŒ ç¼ºå¤±åŠŸèƒ½

**CRITICAL**: **SPLIT_NODEå’ŒRENAME_NODEæ“ä½œæœªå®ç°**

æ–¹æ³•è¯´æ˜è¦æ±‚ä¸‰ç§æ“ä½œ:
- `ADD_NODE`: æ–°å¢åˆ†æ”¯ (cost = 1.0) âœ… å·²å®ç°
- `SPLIT_NODE`: æ‹†åˆ†èŠ‚ç‚¹ (cost = 2.0) âŒ æœªå®ç°
- `RENAME_NODE`: èŠ‚ç‚¹è¯­ä¹‰æ¼‚ç§»åçš„é‡å‘½å (cost = 0.5) âŒ æœªå®ç°

**å½“å‰ä»£ç **:
```python
# phase3_evolution.py, lines 389-392
# Try SPLIT_NODE for overcrowded nodes
# (simplified: skip for now, can add later)  # âŒ TODO comment

# Try RENAME_NODE for drifted nodes
# (simplified: skip for now, can add later)  # âŒ TODO comment
```

**å½±å“**: åªèƒ½å¤„ç†"æ–°å¢ç±»åˆ«"åœºæ™¯,æ— æ³•å¤„ç†"èŠ‚ç‚¹è¿‡åº¦æ‹¥æŒ¤"æˆ–"è¯­ä¹‰æ¼‚ç§»"åœºæ™¯ã€‚

#### âš ï¸ éœ€è¦å¢å¼ºçš„åŠŸèƒ½

1. **FitGainè®¡ç®—è¿‡äºç®€åŒ–**:
   - å½“å‰: `fit_gain = len(cluster.papers) * 0.5` (å›ºå®š50%æ”¹è¿›ç‡)
   - æ–¹æ³•è¯´æ˜: åº”åŸºäºå®é™…re-fitæµ‹è¯•è®¡ç®—æ”¹è¿›
   - **å»ºè®®**: æ¨¡æ‹Ÿåº”ç”¨operationåé‡æ–°æµ‹è¯•fit

2. **Evidenceç»‘å®šä¸å®Œæ•´**:
   - `_extract_cluster_evidence()`åªå–å‰3ç¯‡è®ºæ–‡çš„abstract
   - åº”è¯¥ä»paper claimsçš„evidence_spansæå–

3. **Coverageè®¡ç®—**:
   - å½“å‰: `min(1.0, num_leaves / 50.0)` - ç¡¬ç¼–ç 50
   - **å»ºè®®**: ä½¿ç”¨åŠ¨æ€åŸºå‡† (å¦‚baselineçš„å¹³å‡å¶èŠ‚ç‚¹æ•°)

### 4.3 æ”¹è¿›å»ºè®®

#### **æ”¹è¿›1: å®ç°SPLIT_NODEæ“ä½œ**

```python
def _propose_split_node(
    self,
    cluster: StressCluster,
    view: TaxonomyView,
    fit_vectors: List[FitVector],
    overcrowding_threshold: int = 15  # å¶èŠ‚ç‚¹åŒ…å«>15ç¯‡è®ºæ–‡è§†ä¸ºovercrowded
) -> Optional[SplitNodeOperation]:
    """
    æè®®æ‹†åˆ†overcrowdedèŠ‚ç‚¹ã€‚
    
    è¯†åˆ«æ ‡å‡†:
    1. èŠ‚ç‚¹åŒ…å«çš„è®ºæ–‡æ•° > threshold
    2. è®ºæ–‡ä¹‹é—´ç¼ºä¹cohesion (å†…éƒ¨ç›¸ä¼¼åº¦ä½)
    
    æ‹†åˆ†ç­–ç•¥:
    1. å¯¹èŠ‚ç‚¹å†…è®ºæ–‡èšç±» (k=2æˆ–3)
    2. ä¸ºæ¯ä¸ªå­ç°‡ç”Ÿæˆæ–°çš„sub_nodeå®šä¹‰
    """
    
    # æ‰¾åˆ°overcrowdedçš„å¶èŠ‚ç‚¹
    leaf_paper_count = defaultdict(list)
    for fv in fit_vectors:
        for report in fv.fit_reports:
            if report.view_id == view.view_id and report.best_leaf_path:
                leaf_paper_count[report.best_leaf_path].append(fv.paper_id)
    
    # æ‰¾åˆ°å€™é€‰æ‹†åˆ†èŠ‚ç‚¹
    for leaf_path, paper_ids in leaf_paper_count.items():
        if len(paper_ids) > overcrowding_threshold:
            # æ£€æŸ¥clusterè®ºæ–‡æ˜¯å¦åœ¨è¿™ä¸ªå¶èŠ‚ç‚¹ä¸­
            cluster_paper_ids = {p.url for p in cluster.papers}
            overlap = cluster_paper_ids & set(paper_ids)
            
            if len(overlap) >= 3:  # è‡³å°‘3ç¯‡clusterè®ºæ–‡åœ¨è¿™ä¸ªèŠ‚ç‚¹
                # å¯¹èŠ‚ç‚¹å†…è®ºæ–‡èšç±»
                sub_clusters = self._subcluster_papers(overlap)
                
                # ç”Ÿæˆå­èŠ‚ç‚¹å®šä¹‰
                sub_nodes = []
                for sub_cluster in sub_clusters:
                    sub_node = self._generate_subnode_definition(
                        parent_path=leaf_path,
                        papers=sub_cluster,
                        view=view
                    )
                    sub_nodes.append(sub_node)
                
                # è®¡ç®—fit_gain
                fit_gain = self._estimate_split_fit_gain(leaf_path, sub_nodes, fit_vectors, view)
                
                operation = SplitNodeOperation(
                    view_id=view.view_id,
                    node_path=leaf_path,
                    sub_nodes=sub_nodes,
                    evidence=self._extract_cluster_evidence(cluster),
                    fit_gain=fit_gain
                )
                
                return operation
    
    return None
```

#### **æ”¹è¿›2: å®ç°RENAME_NODEæ“ä½œ**

```python
def _propose_rename_node(
    self,
    cluster: StressCluster,
    view: TaxonomyView,
    fit_vectors: List[FitVector],
    drift_threshold: float = 0.3
) -> Optional[RenameNodeOperation]:
    """
    æè®®é‡å‘½åsemantic driftçš„èŠ‚ç‚¹ã€‚
    
    è¯†åˆ«æ ‡å‡†:
    1. èŠ‚ç‚¹çš„ç°æœ‰å®šä¹‰ä¸å®é™…è®ºæ–‡çš„semantic gap > threshold
    2. å¤§é‡FORCE_FITè®ºæ–‡ (å®šä¹‰è¿‡æ—¶ä½†å‹‰å¼ºå½’ç±»)
    
    ç­–ç•¥:
    1. åˆ†æFORCE_FITè®ºæ–‡çš„lost_novelty
    2. æå–å…±åŒä¸»é¢˜
    3. ç”Ÿæˆæ–°çš„èŠ‚ç‚¹åç§°å’Œå®šä¹‰
    """
    
    # æ‰¾åˆ°FORCE_FITç‡é«˜çš„å¶èŠ‚ç‚¹
    leaf_force_fit = defaultdict(lambda: {'total': 0, 'force_fit': 0, 'novelties': []})
    
    for fv in fit_vectors:
        for report in fv.fit_reports:
            if report.view_id == view.view_id and report.best_leaf_path:
                leaf_force_fit[report.best_leaf_path]['total'] += 1
                if report.label == FitLabel.FORCE_FIT:
                    leaf_force_fit[report.best_leaf_path]['force_fit'] += 1
                    leaf_force_fit[report.best_leaf_path]['novelties'].extend(
                        [ln.bullet for ln in report.lost_novelty]
                    )
    
    # æ‰¾åˆ°driftå€™é€‰
    for leaf_path, stats in leaf_force_fit.items():
        if stats['total'] < 3:
            continue
        
        force_fit_rate = stats['force_fit'] / stats['total']
        
        if force_fit_rate > drift_threshold:
            # è¿™ä¸ªèŠ‚ç‚¹æœ‰è¯­ä¹‰æ¼‚ç§»
            old_def = view.node_definitions.get(leaf_path)
            if not old_def:
                continue
            
            # åˆ†ælost_novelty,ç”Ÿæˆæ–°å®šä¹‰
            new_name, new_def = self._generate_renamed_definition(
                old_def=old_def,
                lost_novelties=stats['novelties']
            )
            
            operation = RenameNodeOperation(
                view_id=view.view_id,
                node_path=leaf_path,
                old_name=old_def.node_path.split('/')[-1],
                new_name=new_name,
                new_definition=new_def,
                drift_score=force_fit_rate,
                evidence=self._extract_cluster_evidence(cluster),
                fit_gain=stats['force_fit'] * 0.3  # å‡è®¾30%æ”¹è¿›
            )
            
            return operation
    
    return None
```

#### **æ”¹è¿›3: å¢å¼ºFitGainè®¡ç®— (çœŸå®Re-fitæµ‹è¯•)**

```python
def _estimate_operation_fit_gain_precise(
    self,
    operation: EvolutionOperation,
    cluster: StressCluster,
    view: TaxonomyView,
    fit_vectors: List[FitVector]
) -> float:
    """
    ç²¾ç¡®ä¼°ç®—operationçš„FitGain: æ¨¡æ‹Ÿåº”ç”¨operationåé‡æ–°fitæµ‹è¯•ã€‚
    
    æ­¥éª¤:
    1. å…‹éš†viewå¹¶åº”ç”¨operation
    2. å¯¹clusterè®ºæ–‡é‡æ–°fitæµ‹è¯•
    3. è®¡ç®—before/afterçš„fit_scoreå·®å¼‚
    """
    
    # å…‹éš†view
    view_copy = copy.deepcopy(view)
    
    # åº”ç”¨operationåˆ°view_copy
    if isinstance(operation, AddNodeOperation):
        # æ·»åŠ æ–°èŠ‚ç‚¹åˆ°tree
        new_path = f"{operation.parent_path}/{operation.new_node.name}"
        new_tree_node = TaxonomyTreeNode(
            name=operation.new_node.name,
            path=new_path,
            parent=operation.parent_path,
            children=[],
            is_leaf=True
        )
        view_copy.tree.add_node(new_tree_node)
        
        # æ·»åŠ èŠ‚ç‚¹å®šä¹‰
        new_node_def = NodeDefinition(
            node_path=new_path,
            definition=operation.new_node.definition,
            inclusion_criteria=operation.new_node.inclusion_criteria,
            exclusion_criteria=operation.new_node.exclusion_criteria,
            canonical_keywords=operation.new_node.keywords,
            boundary_statements=[],
            evidence_spans=operation.evidence
        )
        view_copy.node_definitions[new_path] = new_node_def
    
    # å¯¹clusterè®ºæ–‡é‡æ–°fitæµ‹è¯•
    fit_gain_sum = 0.0
    for paper in cluster.papers:
        # æ‰¾åˆ°åŸå§‹fit_score
        original_fv = next((fv for fv in fit_vectors if fv.paper_id == paper.url), None)
        if not original_fv:
            continue
        
        original_report = next((r for r in original_fv.fit_reports if r.view_id == view.view_id), None)
        if not original_report:
            continue
        
        original_score = original_report.scores.fit_score
        
        # ç”¨view_copyé‡æ–°æµ‹è¯• (éœ€è¦PaperClaims)
        # new_score = self._refit_paper(paper, view_copy)
        # fit_gain_sum += max(0, new_score - original_score)
        
        # Simplified: å‡è®¾æ–°èŠ‚ç‚¹ä½¿UNFITTABLEâ†’FIT (gain=1.0), FORCE_FITâ†’FIT (gain=0.5)
        if original_report.label == FitLabel.UNFITTABLE:
            fit_gain_sum += 1.0
        elif original_report.label == FitLabel.FORCE_FIT:
            fit_gain_sum += 0.5
    
    return fit_gain_sum
```

---

## 5. Phase 4: Delta-aware Guidance - è¯¦ç»†åˆ†æ

### 5.1 æ–¹æ³•è¯´æ˜è¦æ±‚

**ç›®æ ‡**: ç”Ÿæˆå¯å®¡è®¡çš„è®¤çŸ¥å¢é‡èµ„äº§ä¸å¯æ‰§è¡Œçš„å†™ä½œçº¦æŸ,æ”¯æŒä¸‹æ¸¸ç»¼è¿°ç³»ç»Ÿç”Ÿæˆå…·æœ‰ç»“æ„æ€§æ–°è®¤çŸ¥çš„ç»¼è¿°å†…å®¹

**å…³é”®æ­¥éª¤**:
1. **ä¸»è½´/è¾…è½´ç»„ç»‡æ–¹æ¡ˆç¡®å®š (Evolution-first/Reconstruct-then-select)**:
   - å¯¹å€™é€‰è§†è§’æ‰§è¡Œæœ€å°å¿…è¦é‡æ„å¾—åˆ° T_i'
   - è®¡ç®—æ¯è§†è§’çš„ `Score_i = Î±Â·FitGain + Î²Â·Stress + Î³Â·Coverage âˆ’ Î»Â·EditCost`
   - ä¾æ®åˆ†æ•°ä¸è¦†ç›–æ€§é€‰æ‹©ä¸¤ç§ç»„ç»‡æ¨¡å¼ä¹‹ä¸€:
     - **Delta-first**: é‡æ„åçš„é«˜å‹åŠ›è§†è§’ä½œä¸ºä¸»è½´ (EditCost > 3.0 æˆ– FitGain > 10.0)
     - **Anchor+Delta**: è¦†ç›–ç¨³å®šè§†è§’ä½œä¸ºé”šå®šä¸»è½´,é‡æ„è§†è§’ä½œä¸ºè´¯ç©¿è¾…è½´

2. **å†™ä½œçº¦æŸç¼–è¯‘**:
   - **ç»„ç»‡æ¨¡å¼ä¸è½´**: `main_axis_mode`, `main_axis`, `aux_axes`
   - **taxonomy_v2**: æ›´æ–°åçš„ä¸»è½´æ ‘ç»“æ„ + æ¼”åŒ–æ“ä½œåºåˆ—
   - **outline_constraints**: ç« èŠ‚æ ‡é¢˜ã€å¿…è¦†ç›–ç‚¹ã€å¿…ç­”é—®é¢˜ã€å¿…å¼•è®ºæ–‡åˆ—è¡¨
   - **writing_rules (Do/Don't)**: æ˜ç¡®ç¦æ­¢æ»åå†™æ³•,æ˜ç¡®è¯æ®è¦æ±‚

3. **è¾“å‡ºå½¢æ€**:
   - `audit_report.md`: å®¡è®¡æŠ¥å‘Š (äººç±»å¯è¯»)
   - `guidance_pack.json`: æœºå™¨å¯è¯»çº¦æŸåŒ… (ä¸‹æ¸¸å¯æ‰§è¡Œ)

### 5.2 å½“å‰ä»£ç å®ç°

#### âœ… å·²å®ç°åŠŸèƒ½

1. **AxisSelector** (lines 32-223):
   - âœ… **select_main_axis_with_mode()** (NEW DESIGN, lines 45-112):
     - âœ… åŸºäºreconstruction_scoresé€‰æ‹©main_axis
     - âœ… ç¡®å®šwriting_mode (DELTA_FIRST vs ANCHOR_PLUS_DELTA)
     - âœ… é˜ˆå€¼è§„åˆ™: `EditCost > 3.0 or FitGain > 10.0 â†’ DELTA_FIRST`
   - âœ… **select_aux_axis()** (lines 165-223):
     - âœ… åŸºäºdiscriminativeness (variance of failure rates)

2. **GuidanceGenerator** (lines 225-602):
   - âœ… **generate_guidance()** (lines 233-312):
     - âœ… åŒ…å« main_axis_mode, writing_rules, reconstruction_scores
   - âœ… **_generate_outline()** (lines 314-384):
     - âœ… åŸºäºmain_axis treeç»“æ„ç”Ÿæˆsections
     - âœ… Cross-organize with aux_axis (subsections)
   - âœ… **_generate_writing_rules()** (lines 482-549):
     - âœ… Mode-specific rules:
       - DELTA_FIRST: "Lead with evolution", "Don't force-fit"
       - ANCHOR_PLUS_DELTA: "Use structure", "Mark updates"
   - âœ… **_generate_evolution_summary()** (lines 551-570)
   - âœ… **_generate_must_answer_questions()** (lines 572-601)

3. **SNSRunner._save_guidance_pack()** (engine_v2.py, lines 442-516):
   - âœ… ç”Ÿæˆ `guidance_pack.json` (machine-readable)
   - âœ… åŒ…å«: writing_mode, writing_rules, taxonomy, outline, evolution_summary, must_answer_questions, reconstruction_scores

4. **SNSRunner._generate_markdown_report()** (engine_v2.py, lines 518-606):
   - âœ… ç”Ÿæˆ `audit_report.md` (human-readable)

#### âš ï¸ éœ€è¦å¢å¼ºçš„åŠŸèƒ½

1. **taxonomy_v2 (æ¼”åŒ–åçš„taxonomy)æœªæ˜ç¡®è¾“å‡º**:
   - æ–¹æ³•è¦æ±‚: "taxonomy_v2: æ›´æ–°åçš„ä¸»è½´æ ‘ç»“æ„ä¸èŠ‚ç‚¹å®šä¹‰ + æ¼”åŒ–æ“ä½œåºåˆ—"
   - å½“å‰: `guidance_pack.json` åŒ…å«main_axis.tree,ä½†**æ²¡æœ‰åº”ç”¨evolution operations**
   - **é—®é¢˜**: taxonomyä»ç„¶æ˜¯Phase 1çš„åŸå§‹ç‰ˆæœ¬,æ²¡æœ‰ADD_NODE/SPLIT_NODEçš„æ›´æ–°
   - **å»ºè®®**: åœ¨Phase 4å¼€å§‹å‰,å°†accepted operationsåº”ç”¨åˆ°view.tree

2. **Outline constraintsä¸å¤Ÿå…·ä½“**:
   - æ–¹æ³•è¦æ±‚: "å¿…è¦†ç›–ç‚¹ã€å¿…ç­”é—®é¢˜ã€å¿…å¼•è¯æ®"
   - å½“å‰: `must_answer`é—®é¢˜è¿‡äºé€šç”¨ ("What are the key approaches?")
   - **å»ºè®®**: ä»evolutionæ“ä½œå’Œclusteråˆ†æç”Ÿæˆæ›´å…·ä½“çš„é—®é¢˜

3. **Evidence Cardsè´¨é‡**:
   - å½“å‰: Evidence cardsåªåŒ…å«paper titleå’Œabstractå‰200å­—
   - æ–¹æ³•å¼ºè°ƒ: "æ¯é¡¹æ“ä½œçš„è§¦å‘ç°‡ä¸è¯æ®å¡"
   - **å»ºè®®**: ä»fit_reportsçš„lost_noveltyå’Œconflict_evidenceæå–ç²¾ç¡®quotes

### 5.3 æ”¹è¿›å»ºè®®

#### **æ”¹è¿›1: åº”ç”¨Evolution Operationsåˆ°Taxonomy**

```python
def apply_evolution_to_taxonomy(
    view: TaxonomyView,
    operations: List[EvolutionOperation]
) -> TaxonomyView:
    """
    å°†accepted evolution operationsåº”ç”¨åˆ°taxonomy tree,ç”Ÿæˆtaxonomy_v2ã€‚
    
    æ”¯æŒ:
    - ADD_NODE: æ·»åŠ æ–°å¶èŠ‚ç‚¹
    - SPLIT_NODE: æ‹†åˆ†ç°æœ‰èŠ‚ç‚¹ä¸ºå¤šä¸ªå­èŠ‚ç‚¹
    - RENAME_NODE: é‡å‘½åèŠ‚ç‚¹åŠæ›´æ–°å®šä¹‰
    """
    
    view_v2 = copy.deepcopy(view)
    
    for op in operations:
        if isinstance(op, AddNodeOperation):
            # æ·»åŠ æ–°èŠ‚ç‚¹
            new_path = f"{op.parent_path}/{op.new_node.name}"
            new_node = TaxonomyTreeNode(
                name=op.new_node.name,
                path=new_path,
                parent=op.parent_path,
                children=[],
                is_leaf=True
            )
            view_v2.tree.add_node(new_node)
            
            # æ·»åŠ èŠ‚ç‚¹å®šä¹‰
            new_def = NodeDefinition(
                node_path=new_path,
                definition=op.new_node.definition,
                inclusion_criteria=op.new_node.inclusion_criteria,
                exclusion_criteria=op.new_node.exclusion_criteria,
                canonical_keywords=op.new_node.keywords,
                boundary_statements=[],
                evidence_spans=op.evidence
            )
            view_v2.node_definitions[new_path] = new_def
        
        elif isinstance(op, SplitNodeOperation):
            # æ‹†åˆ†èŠ‚ç‚¹
            parent_node = view_v2.tree.nodes[op.node_path]
            parent_node.is_leaf = False  # å˜ä¸ºå†…éƒ¨èŠ‚ç‚¹
            
            for sub_node_prop in op.sub_nodes:
                sub_path = f"{op.node_path}/{sub_node_prop.name}"
                sub_node = TaxonomyTreeNode(
                    name=sub_node_prop.name,
                    path=sub_path,
                    parent=op.node_path,
                    children=[],
                    is_leaf=True
                )
                view_v2.tree.add_node(sub_node)
                
                # æ·»åŠ å®šä¹‰
                sub_def = NodeDefinition(
                    node_path=sub_path,
                    definition=sub_node_prop.definition,
                    inclusion_criteria=sub_node_prop.inclusion_criteria,
                    exclusion_criteria=sub_node_prop.exclusion_criteria,
                    canonical_keywords=sub_node_prop.keywords,
                    boundary_statements=[],
                    evidence_spans=op.evidence
                )
                view_v2.node_definitions[sub_path] = sub_def
        
        elif isinstance(op, RenameNodeOperation):
            # é‡å‘½åèŠ‚ç‚¹
            node = view_v2.tree.nodes[op.node_path]
            old_name = node.name
            node.name = op.new_name
            
            # æ›´æ–°path (éœ€è¦é€’å½’æ›´æ–°å­èŠ‚ç‚¹)
            # ... (å®ç°pathæ›´æ–°é€»è¾‘)
            
            # æ›´æ–°å®šä¹‰
            if op.node_path in view_v2.node_definitions:
                view_v2.node_definitions[op.node_path].definition = op.new_definition
    
    return view_v2
```

**é›†æˆåˆ°Phase4Pipeline**:
```python
def run(self, ...):
    # ...
    
    # åœ¨axis selectionä¹‹å‰,åº”ç”¨evolutionåˆ°main_axis
    main_axis_v2 = apply_evolution_to_taxonomy(main_axis, evolution_proposal.operations)
    
    guidance = self.guidance_generator.generate_guidance(
        # ...
        main_axis=main_axis_v2,  # ä½¿ç”¨evolvedç‰ˆæœ¬
        # ...
    )
```

#### **æ”¹è¿›2: ç”ŸæˆDelta-aware Must-answer Questions**

```python
def _generate_must_answer_questions_enhanced(
    self,
    main_axis: TaxonomyView,
    aux_axis: Optional[TaxonomyView],
    clusters: List[StressCluster],
    proposal: EvolutionProposal,
    baseline: MultiViewBaseline
) -> List[str]:
    """
    ç”Ÿæˆå…·ä½“çš„å¿…ç­”é—®é¢˜,ç›´æ¥å…³è”åˆ°evolutionå’Œstress pointsã€‚
    """
    
    questions = []
    
    # 1. åŸºç¡€ç»“æ„é—®é¢˜
    questions.append(
        f"What are the main organizational dimensions in {main_axis.facet_label.value}?"
    )
    
    # 2. æ¼”åŒ–æ“ä½œç›¸å…³é—®é¢˜ (æ¯ä¸ªoperationä¸€ä¸ªé—®é¢˜)
    for op in proposal.operations:
        if isinstance(op, AddNodeOperation):
            questions.append(
                f"Why was the new category '{op.new_node.name}' needed in {op.view_id}? "
                f"What papers don't fit existing structure?"
            )
        elif isinstance(op, SplitNodeOperation):
            questions.append(
                f"Why was '{op.node_path}' split into subcategories? "
                f"What overcrowding or heterogeneity was observed?"
            )
        elif isinstance(op, RenameNodeOperation):
            questions.append(
                f"Why was '{op.old_name}' renamed to '{op.new_name}'? "
                f"What semantic drift occurred?"
            )
    
    # 3. Stress clusterç›¸å…³é—®é¢˜
    for cluster in clusters:
        if cluster.cluster_type == ClusterType.STRONG_SHIFT:
            # æ‰¾åˆ°clusterä¸­æœ€é«˜å‹åŠ›çš„è§†è§’
            max_failure_view_id = max(
                cluster.view_failure_rates.items(),
                key=lambda x: x[1]
            )[0]
            view = baseline.get_view_by_id(max_failure_view_id)
            
            questions.append(
                f"Cluster {cluster.cluster_id} shows strong structural shift: "
                f"How do these {len(cluster.papers)} papers challenge "
                f"{view.facet_label.value if view else 'existing'} organization? "
                f"What new patterns emerge?"
            )
    
    # 4. æ—§ç»“æ„ä¸è¶³è¯æ®é—®é¢˜ (å…³é”®!)
    questions.append(
        "What evidence demonstrates that existing taxonomies are insufficient "
        "for organizing recent research? Cite specific FORCE_FIT or UNFITTABLE cases."
    )
    
    # 5. è¾…è½´discriminativenessé—®é¢˜
    if aux_axis:
        questions.append(
            f"How does {aux_axis.facet_label.value} provide orthogonal perspective? "
            f"Which stress clusters does it help discriminate?"
        )
    
    return questions
```

---

## 6. ç¼ºå¤±åŠŸèƒ½æ€»ç»“ä¸ä¼˜å…ˆçº§

### ğŸ”´ Critical (å¿…é¡»å®ç°)

| åŠŸèƒ½ | Phase | å½“å‰çŠ¶æ€ | å½±å“ |
|-----|-------|---------|------|
| **è¡¥è§†è§’ç­–ç•¥ (CompensatoryViewInducer)** | Phase 1 | âŒ æœªå®ç° | baselineè´¨é‡ä¸è¶³æ—¶æ— æ³•è‡ªåŠ¨è¡¥æ•‘ |
| **NLIå†²çªæ£€æµ‹ (NLIConflictDetector)** | Phase 2 | âŒ æœªå®ç° | Conflictåˆ†æ•°ä¸å‡†ç¡®,FITåˆ¤å®šå¯èƒ½é”™è¯¯ |
| **çœŸå®Embeddingæ¨¡å‹ (SPECTER2/SciNCL)** | Phase 2 | âŒ æœªå®ç° | Coverage/Residualåˆ†æ•°ä¸å‡†ç¡® |
| **SPLIT_NODEæ“ä½œ** | Phase 3 | âŒ æœªå®ç° | æ— æ³•å¤„ç†èŠ‚ç‚¹overcrowding |
| **RENAME_NODEæ“ä½œ** | Phase 3 | âŒ æœªå®ç° | æ— æ³•å¤„ç†semantic drift |
| **åº”ç”¨Evolutionåˆ°Taxonomy_v2** | Phase 4 | âš ï¸ éƒ¨åˆ†å®ç° | è¾“å‡ºçš„taxonomyæœªåŒ…å«ç»“æ„æ›´æ–° |

### ğŸŸ¡ High Priority (åº”è¯¥å®ç°)

| åŠŸèƒ½ | Phase | å½“å‰çŠ¶æ€ | å½±å“ |
|-----|-------|---------|------|
| **èŠ‚ç‚¹å®šä¹‰è´¨é‡éªŒè¯** | Phase 1 | âš ï¸ æ— éªŒè¯ | å¯èƒ½ç”Ÿæˆä¸è¶³3/2æ ‡å‡†çš„å®šä¹‰ |
| **Evidence Spanç²¾ç¡®æ€§éªŒè¯** | Phase 1-4 | âš ï¸ ä¾èµ–LLM | char_start/endå¯èƒ½ä¸å‡†ç¡® |
| **é˜ˆå€¼å¯æ ¡å‡†æ¥å£** | Phase 2 | âš ï¸ ç¡¬ç¼–ç  | æ— æ³•é€‚é…ä¸åŒé¢†åŸŸ |
| **FitGainç²¾ç¡®è®¡ç®— (re-fitæµ‹è¯•)** | Phase 3 | âš ï¸ ç®€åŒ–ä¼°ç®— | operation benefitä¼°ç®—ä¸å‡† |
| **Delta-aware Must-answer Questions** | Phase 4 | âš ï¸ è¿‡äºé€šç”¨ | é—®é¢˜ç¼ºä¹é’ˆå¯¹æ€§ |
| **Evidence Cardsè´¨é‡å¢å¼º** | Phase 4 | âš ï¸ åªæœ‰abstract | ç¼ºä¹ç²¾ç¡®quotes |

### ğŸŸ¢ Medium Priority (å»ºè®®å®ç°)

| åŠŸèƒ½ | Phase | å½“å‰çŠ¶æ€ | å½±å“ |
|-----|-------|---------|------|
| **æƒé‡è®¡ç®—çš„Qualityå› å­** | Phase 1 | âš ï¸ ç®€åŒ– | ä½¿ç”¨æ–‡æœ¬é•¿åº¦ä»£ç†citations |
| **å¤šfacetè¡¥è§†è§’å¤šæ ·æ€§** | Phase 1 | âš ï¸ å•æ¬¡è¡¥æ•‘ | å¯èƒ½éœ€è¦å¤šè½®è¡¥è§†è§’ |
| **Clusterå†…cohesionæ£€æŸ¥** | Phase 3 | âŒ æœªå®ç° | èšç±»è´¨é‡æœªéªŒè¯ |
| **åŠ¨æ€CoverageåŸºå‡†** | Phase 3 | âš ï¸ ç¡¬ç¼–ç 50 | ä¸åŒé¢†åŸŸtaxonomyå¤§å°å·®å¼‚å¤§ |

---

## 7. å®ç°æ”¹è¿›è·¯çº¿å›¾

### Phase 1: æ ¸å¿ƒåŠŸèƒ½è¡¥é½ (Week 1-2)

1. **é›†æˆçœŸå®Embeddingæ¨¡å‹**:
   - æ·»åŠ  `ScientificEmbedder` ç±» (SPECTER2)
   - æ›´æ–° `EmbeddingBasedRetriever`
   - æ›´æ–° `FitTester._calculate_coverage()` å’Œ `._calculate_residual()`

2. **é›†æˆNLIå†²çªæ£€æµ‹**:
   - æ·»åŠ  `NLIConflictDetector` ç±» (DeBERTa-MNLI)
   - æ›´æ–° `FitTester._calculate_conflict()`
   - æ·»åŠ fallbackæœºåˆ¶

3. **å®ç°è¡¥è§†è§’ç­–ç•¥**:
   - æ·»åŠ  `CompensatoryViewInducer` ç±»
   - é›†æˆåˆ° `Phase1Pipeline.run()` çš„quality gateå

### Phase 2: Evolutionæ“ä½œå®Œæ•´åŒ– (Week 3)

4. **å®ç°SPLIT_NODE**:
   - æ·»åŠ  `_propose_split_node()` åˆ° `EvolutionPlanner`
   - å®ç°èŠ‚ç‚¹å†…è®ºæ–‡èšç±»é€»è¾‘
   - ç”ŸæˆLLM prompt for subnode definition

5. **å®ç°RENAME_NODE**:
   - æ·»åŠ  `_propose_rename_node()` åˆ° `EvolutionPlanner`
   - æ£€æµ‹FORCE_FITç‡é«˜çš„èŠ‚ç‚¹
   - åˆ†ælost_noveltyç”Ÿæˆæ–°å®šä¹‰

6. **å¢å¼ºFitGainè®¡ç®—**:
   - å®ç° `_estimate_operation_fit_gain_precise()`
   - æ¨¡æ‹Ÿåº”ç”¨operationåre-fitæµ‹è¯•

### Phase 3: Guidanceè´¨é‡æå‡ (Week 4)

7. **åº”ç”¨Evolutionåˆ°Taxonomy**:
   - å®ç° `apply_evolution_to_taxonomy()`
   - åœ¨Phase 4ä¸­ä½¿ç”¨taxonomy_v2

8. **å¢å¼ºMust-answer Questions**:
   - å®ç° `_generate_must_answer_questions_enhanced()`
   - æ¯ä¸ªoperationç”Ÿæˆå…·ä½“é—®é¢˜
   - æ¯ä¸ªSTRONG_SHIFT clusterç”Ÿæˆé—®é¢˜

9. **æå‡Evidence Cardsè´¨é‡**:
   - ä»fit_reportsæå–lost_novelty quotes
   - ä»conflict_evidenceæå–ç²¾ç¡®spans

### Phase 4: è´¨é‡ä¿éšœä¸éªŒè¯ (Week 5)

10. **æ·»åŠ èŠ‚ç‚¹å®šä¹‰éªŒè¯**:
    - å®ç° `_validate_node_definition()`
    - SchemaéªŒè¯ (â‰¥3 inclusion, â‰¥2 exclusion)
    - Evidence spanså®Œæ•´æ€§æ£€æŸ¥

11. **é˜ˆå€¼å¯é…ç½®åŒ–**:
    - å°†0.45, 0.55ç­‰é˜ˆå€¼ç§»åˆ°SNSArguments
    - æ·»åŠ per-domain calibrationæ¥å£

12. **é›†æˆæµ‹è¯•**:
    - ç«¯åˆ°ç«¯æµ‹è¯• (å®Œæ•´pipeline)
    - éªŒè¯guidance_pack.jsonæ ¼å¼
    - éªŒè¯audit_report.mdå¯è¯»æ€§

---

## 8. å…³é”®è®¾è®¡å†³ç­–ä¸å¯¹é½

### âœ… è®¾è®¡å†³ç­–å¯¹é½è‰¯å¥½

1. **Reconstruct-then-select (Phase 3-4)**:
   - æ–¹æ³•è¦æ±‚: "å…ˆé‡æ„å†é€‰æ‹©"
   - ä»£ç å®ç°: âœ… `compute_all_views_reconstruction()` â†’ `select_main_axis_with_mode()`

2. **Writing Mode Determination**:
   - æ–¹æ³•è¦æ±‚: `EditCost > 3.0 or FitGain > 10.0 â†’ DELTA_FIRST`
   - ä»£ç å®ç°: âœ… å®Œå…¨ä¸€è‡´ (phase4_guidance.py, lines 94-101)

3. **Multi-view Atlas**:
   - æ–¹æ³•è¦æ±‚: "å¤šè§†è§’å›¾é›†,æ¯ä¸ªè§†è§’ç‹¬ç«‹æ ‘ç»“æ„"
   - ä»£ç å®ç°: âœ… `MultiViewBaseline` + `TaxonomyView` + `TaxonomyTree`

4. **Evidence Anchoring**:
   - æ–¹æ³•è¦æ±‚: "æ‰€æœ‰claimç»‘å®šåŸæ–‡spans"
   - ä»£ç å®ç°: âœ… `EvidenceSpan` æ•°æ®ç»“æ„åŒ…å« char_start, char_end, quote

5. **Deterministic Scoring**:
   - æ–¹æ³•è¦æ±‚: "å…³é”®å†³ç­–(FIT/FORCE_FIT/UNFITTABLE)ç”±ç¡®å®šæ€§è§„åˆ™"
   - ä»£ç å®ç°: âœ… é˜ˆå€¼è§„åˆ™ (0.45, 0.55) in `FitTester._determine_label()`

### âš ï¸ è®¾è®¡åç¦» (éœ€è¦è§£é‡Šæˆ–æ”¹è¿›)

1. **LLM Temperature**:
   - æ–¹æ³•è¦æ±‚: "temperature=0 for reproducibility"
   - ä»£ç å®ç°: â“ ä½¿ç”¨ `dspy.context(lm=self.lm)` ä½†æœªæ˜¾å¼è®¾ç½®temperature
   - **å»ºè®®**: åœ¨LMåˆå§‹åŒ–æ—¶å¼ºåˆ¶ `temperature=0`

2. **Novelty Bulletsæ•°é‡**:
   - æ–¹æ³•è¦æ±‚: "Must have exactly 3"
   - ä»£ç å®ç°: âœ… Enforced with padding/trimming (phase2, lines 83-91)

3. **Weight Normalization**:
   - æ–¹æ³•è¦æ±‚: w_iå½’ä¸€åŒ–
   - ä»£ç å®ç°: âœ… åœ¨ `MultiViewBaseline.__post_init__` (lines 224-229)

---

## 9. ç»“è®ºä¸å»ºè®®

### 9.1 æ€»ä½“è¯„ä¼°

**å®ç°å®Œæ•´åº¦**: **70%**

- âœ… **æ ¸å¿ƒæ¡†æ¶**: å®Œæ•´å®ç° (Phase 1-4 pipeline, æ•°æ®ç»“æ„)
- âœ… **å…³é”®è®¾è®¡**: Reconstruct-then-select, Writing mode, Multi-view atlas
- âŒ **ç¼ºå¤±CriticalåŠŸèƒ½**: è¡¥è§†è§’ã€NLIã€Embeddingsã€SPLIT/RENAMEã€Taxonomy_v2åº”ç”¨
- âš ï¸ **è´¨é‡å¾…æå‡**: Evidenceç²¾ç¡®æ€§ã€FitGainä¼°ç®—ã€Must-answeré—®é¢˜

### 9.2 ä¼˜å…ˆæ”¹è¿›å»ºè®®

**ç«‹å³ç€æ‰‹ (Week 1)**:
1. é›†æˆSPECTER2 embeddings (å½±å“æ‰€æœ‰ç›¸ä¼¼åº¦è®¡ç®—)
2. é›†æˆNLIå†²çªæ£€æµ‹ (å½±å“FITåˆ¤å®šå‡†ç¡®æ€§)
3. å®ç°è¡¥è§†è§’ç­–ç•¥ (ä¿è¯baselineè´¨é‡)

**çŸ­æœŸç›®æ ‡ (Week 2-3)**:
4. å®ç°SPLIT_NODEå’ŒRENAME_NODE
5. å¢å¼ºFitGainè®¡ç®— (re-fitæµ‹è¯•)
6. åº”ç”¨Evolutionåˆ°Taxonomy_v2

**ä¸­æœŸç›®æ ‡ (Week 4-5)**:
7. æå‡Guidanceè´¨é‡ (questions, evidence cards)
8. æ·»åŠ è´¨é‡éªŒè¯ (schema, thresholds)
9. ç«¯åˆ°ç«¯æµ‹è¯•ä¸æ–‡æ¡£

### 9.3 æ–¹æ³•è¯´æ˜é€‚é…æ€§è¯„ä¼°

**å¯¹æ–¹æ³•è¯´æ˜çš„éµå¾ªåº¦**: **Good (85%)**

- âœ… æ‰€æœ‰æ ¸å¿ƒæ¦‚å¿µéƒ½æœ‰å¯¹åº”å®ç°
- âœ… 4ä¸ªPhaseæµç¨‹å®Œæ•´
- âœ… æ•°æ®ç»“æ„ä¸æ–¹æ³•è¯´æ˜ä¸€è‡´
- âŒ éƒ¨åˆ†ç»†èŠ‚åŠŸèƒ½ç¼ºå¤± (ä½†è®¾è®¡ç©ºé—´å·²é¢„ç•™)
- âš ï¸ ä¸€äº›å®ç°ä¸ºplaceholder (ä½†æœ‰æ¸…æ™°TODOæ³¨é‡Š)

**å»ºè®®**: å½“å‰ä»£ç æ¶æ„è‰¯å¥½,é€‚åˆæ¸è¿›å¼è¡¥é½åŠŸèƒ½,æ— éœ€å¤§è§„æ¨¡é‡æ„ã€‚

---

## é™„å½•: æ–‡ä»¶ä¿®æ”¹æ¸…å•

### éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶

| æ–‡ä»¶ | ä¿®æ”¹ç±»å‹ | ä¼˜å…ˆçº§ |
|-----|---------|--------|
| `phase1_multiview_baseline.py` | æ·»åŠ CompensatoryViewInducerç±» | ğŸ”´ Critical |
| `phase2_stress_test.py` | æ·»åŠ NLIConflictDetectorå’ŒScientificEmbedder | ğŸ”´ Critical |
| `phase3_evolution.py` | æ·»åŠ SPLIT/RENAMEæ“ä½œ | ğŸ”´ Critical |
| `phase4_guidance.py` | æ·»åŠ apply_evolutionå’Œenhanced questions | ğŸ”´ Critical |
| `engine_v2.py` | é›†æˆæ–°ç»„ä»¶åˆ°pipeline | ğŸ”´ Critical |
| `dataclass_v2.py` | (å¯èƒ½éœ€è¦æ–°æ•°æ®ç»“æ„) | ğŸŸ¡ High |
| `schemas_v2.py` | æ·»åŠ LLM prompts for SPLIT/RENAME | ğŸŸ¡ High |

### éœ€è¦æ–°å¢çš„æ–‡ä»¶

| æ–‡ä»¶ | ç”¨é€” | ä¼˜å…ˆçº§ |
|-----|-----|--------|
| `knowledge/sns/infrastructure/embeddings_real.py` | SPECTER2/SciNCLå®ç° | ğŸ”´ Critical |
| `knowledge/sns/infrastructure/nli_real.py` | DeBERTa-MNLIå®ç° | ğŸ”´ Critical |
| `knowledge/sns/modules/compensatory_view.py` | è¡¥è§†è§’ç­–ç•¥å®ç° | ğŸ”´ Critical |

---

**æŠ¥å‘Šç”Ÿæˆæ—¥æœŸ**: 2025-12-15  
**åˆ†æäººå‘˜**: Claude (AI Code Assistant)  
**é¡¹ç›®è·¯å¾„**: `/home/user/webapp`
