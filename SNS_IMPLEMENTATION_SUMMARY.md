# SNSæ–¹æ³•å®ç°åˆ†æä¸æ”¹è¿›æ€»ç»“

## æ‰§è¡Œæ¦‚è¦

**åˆ†ææ—¥æœŸ**: 2025-12-15  
**é¡¹ç›®**: SNS (Self-Nonself) for Automatic Survey Generation  
**ä»£ç åº“è·¯å¾„**: `/home/user/webapp`

---

## 1. æ ¸å¿ƒå‘ç°

### âœ… å·²å®ç°çš„ä¼˜ç§€éƒ¨åˆ†

1. **å®Œæ•´çš„æ•°æ®ç»“æ„ä½“ç³»** (`dataclass_v2.py`)
   - æ‰€æœ‰æ ¸å¿ƒæ•°æ®ç±»å·²å®šä¹‰: `MultiViewBaseline`, `FitVector`, `StressCluster`, `EvolutionProposal`, `DeltaAwareGuidance`
   - åŒ…å«æ–¹æ³•è¯´æ˜è¦æ±‚çš„æ‰€æœ‰å­—æ®µ
   - æ”¯æŒåºåˆ—åŒ–/ååºåˆ—åŒ–

2. **Phase 1-4 Pipelineæ¶æ„** (`engine_v2.py`, `phase1-4_*.py`)
   - 4ä¸ªPhaseçš„æµç¨‹æ¡†æ¶å®Œæ•´
   - Pipeline orchestrationæ¸…æ™°
   - ä¸­é—´ç»“æœä¿å­˜æœºåˆ¶å®Œå–„

3. **EmbeddingsåŸºç¡€è®¾æ–½** (`embeddings.py`) âœ… **å·²å­˜åœ¨**
   - SPECTER2, SciNCL, Sentence-BERTå®ç°
   - Fallbackæœºåˆ¶ (TF-IDF)
   - Hybrid similarity (semantic + lexical)

4. **NLIåŸºç¡€è®¾æ–½** (`nli.py`) âœ… **å·²å­˜åœ¨**
   - DeBERTa-MNLIå®ç°
   - Batch processingæ”¯æŒ
   - Fallbackæœºåˆ¶ (keyword-based)

5. **å…³é”®è®¾è®¡å†³ç­–**
   - Reconstruct-then-select: âœ… å®Œå…¨å®ç°
   - Writing Mode determination: âœ… ç¬¦åˆæ–¹æ³•è¯´æ˜
   - Multi-view Atlas: âœ… æ¶æ„æ­£ç¡®
   - Evidence anchoring: âœ… æ•°æ®ç»“æ„æ”¯æŒ

### âš ï¸ éœ€è¦ä¿®å¤çš„å…³é”®é—®é¢˜

#### ğŸ”´ Critical Issue 1: Phase 2æœªä½¿ç”¨çœŸå®Embeddingså’ŒNLI

**ç°çŠ¶**: `phase2_stress_test.py` ä½¿ç”¨placeholderå®ç°
- `EmbeddingBasedRetriever`: keyword overlap (line 214-223)
- `FitTester._calculate_conflict()`: keyword-based (line 335-347)

**é—®é¢˜**: å¯¼è‡´Coverageå’ŒConflictåˆ†æ•°ä¸å‡†ç¡®,FITåˆ¤å®šå¯èƒ½é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**: å·²æœ‰`embeddings.py`å’Œ`nli.py`,éœ€è¦é›†æˆåˆ°Phase 2

#### ğŸ”´ Critical Issue 2: è¡¥è§†è§’ç­–ç•¥æœªå®ç°

**ç°çŠ¶**: `phase1_multiview_baseline.py` åªwarning,æ²¡æœ‰è¡¥æ•‘ (line 537-538)

**é—®é¢˜**: baselineè´¨é‡ä¸è¶³æ—¶æ— æ³•è‡ªåŠ¨æ¢å¤

**è§£å†³æ–¹æ¡ˆ**: éœ€è¦å®ç°`CompensatoryViewInducer`ç±»

#### ğŸ”´ Critical Issue 3: SPLIT_NODEå’ŒRENAME_NODEæœªå®ç°

**ç°çŠ¶**: `phase3_evolution.py` åªæœ‰TODOæ³¨é‡Š (line 389-392)

**é—®é¢˜**: åªèƒ½å¤„ç†ADDåœºæ™¯,æ— æ³•å¤„ç†overcrowdingå’Œsemantic drift

**è§£å†³æ–¹æ¡ˆ**: éœ€è¦å®ç°è¿™ä¸¤ä¸ªoperationç±»å‹

#### ğŸ”´ Critical Issue 4: Taxonomy_v2æœªåº”ç”¨Evolution

**ç°çŠ¶**: `guidance_pack.json`è¾“å‡ºçš„taxonomyæ˜¯åŸå§‹ç‰ˆæœ¬,æœªåŒ…å«evolution operations

**é—®é¢˜**: ä¸‹æ¸¸ç³»ç»Ÿçœ‹ä¸åˆ°ç»“æ„æ›´æ–°

**è§£å†³æ–¹æ¡ˆ**: åœ¨Phase 4å‰åº”ç”¨operationsåˆ°taxonomy tree

---

## 2. è¯¦ç»†åˆ†æ - æŒ‰Phase

### Phase 1: Multi-view Baseline Construction

**å®ç°åº¦**: 85%

| åŠŸèƒ½ | æ–¹æ³•è¦æ±‚ | ä»£ç å®ç° | çŠ¶æ€ | ä¼˜å…ˆçº§ |
|-----|---------|---------|------|--------|
| ç»¼è¿°æ£€ç´¢ | review/survey/tutorialå…³é”®è¯ | âœ… ReviewRetriever | âœ… å®Œæˆ | - |
| TaxonomyæŠ½å– | LLM JSON schema, temperature=0 | âœ… TaxonomyViewExtractor | âœ… å®Œæˆ | - |
| Facetæ ‡ç­¾ | å—æ§æšä¸¾é›† | âœ… FacetLabel enum | âœ… å®Œæˆ | - |
| èŠ‚ç‚¹å®šä¹‰ | inclusionâ‰¥3, exclusionâ‰¥2, evidence_spans | âœ… NodeDefinitionBuilder | âš ï¸ æ— éªŒè¯ | ğŸŸ¡ High |
| æƒé‡è®¡ç®— | w_i âˆ RecencyÃ—QualityÃ—Coverage | âœ… _calculate_weight() | âœ… å®Œæˆ | - |
| è´¨é‡é—¸é—¨ | unique(facet)<2 æ£€æŸ¥ | âœ… _check_baseline_quality() | âœ… å®Œæˆ | - |
| **è¡¥è§†è§’ç­–ç•¥** | **è¯±å¯¼T_extra** | **âŒ åªwarning** | **âŒ ç¼ºå¤±** | **ğŸ”´ Critical** |

**å…³é”®ä»£ç ä½ç½®**:
- `knowledge/sns/modules/phase1_multiview_baseline.py`
- Lines 515-552: quality gateæœ‰æ£€æŸ¥ä½†æ— è¡¥æ•‘
- **éœ€è¦æ·»åŠ **: `CompensatoryViewInducer` ç±»

### Phase 2: Multi-view Stress Test

**å®ç°åº¦**: 70%

| åŠŸèƒ½ | æ–¹æ³•è¦æ±‚ | ä»£ç å®ç° | çŠ¶æ€ | ä¼˜å…ˆçº§ |
|-----|---------|---------|------|--------|
| ClaimæŠ½å– | problem, core_idea, mechanism, training, evaluation, novelty_bullets=3 | âœ… PaperClaimExtractor | âœ… å®Œæˆ | - |
| å€™é€‰å¬å› | Embeddingç›¸ä¼¼åº¦ + Top-K | âš ï¸ keyword overlap | âŒ Placeholder | ğŸ”´ Critical |
| Coverageè®¡ç®— | 0.7Ã—cos(emb) + 0.3Ã—Jaccard | âš ï¸ keyword-based semantic | âŒ Placeholder | ğŸ”´ Critical |
| **Conflictè®¡ç®—** | **max P_NLI(contradiction)** | **âŒ keyword overlap** | **âŒ Placeholder** | **ğŸ”´ Critical** |
| Residualè®¡ç®— | 1 - max cos(novelty, leaf) | âš ï¸ keyword-based | âŒ Placeholder | ğŸ”´ Critical |
| æ ‡ç­¾åˆ¤å®š | é˜ˆå€¼è§„åˆ™ (0.45, 0.55, 0.45) | âœ… _determine_label() | âœ… å®Œæˆ | - |
| Evidenceæå– | lost_novelty, conflict_evidence | âœ… æ•°æ®ç»“æ„ | âœ… å®Œæˆ | - |

**å…³é”®ä»£ç ä½ç½®**:
- `knowledge/sns/modules/phase2_stress_test.py`
- Lines 146-224: `EmbeddingBasedRetriever` - éœ€è¦é›†æˆçœŸå®embeddings
- Lines 309-347: `_calculate_conflict()` - éœ€è¦é›†æˆNLI
- **å·²æœ‰åŸºç¡€è®¾æ–½**: `knowledge/sns/embeddings.py`, `knowledge/sns/nli.py`

**éœ€è¦åšçš„ä¿®æ”¹**:
1. `EmbeddingBasedRetriever.__init__()`: ä½¿ç”¨`create_embedding_model()`
2. `EmbeddingBasedRetriever._compute_similarity()`: ä½¿ç”¨`embedder.similarity()`
3. `FitTester.__init__()`: æ·»åŠ `nli_model`å‚æ•°
4. `FitTester._calculate_conflict()`: ä½¿ç”¨`nli_model.compute_max_conflict_score()`

### Phase 3: Stress Clustering & Evolution

**å®ç°åº¦**: 75%

| åŠŸèƒ½ | æ–¹æ³•è¦æ±‚ | ä»£ç å®ç° | çŠ¶æ€ | ä¼˜å…ˆçº§ |
|-----|---------|---------|------|--------|
| å‹åŠ›ç­›é€‰ | stress_score > 0.3 | âœ… StressClusterer | âœ… å®Œæˆ | - |
| å¤±è´¥ç­¾å | facet + leaf_path + lost_novelty | âœ… _construct_failure_signature() | âœ… å®Œæˆ | - |
| HDBSCANèšç±» | æ— éœ€æŒ‡å®šK | âœ… _cluster_signatures() | âœ… å®Œæˆ | - |
| ClusterTypeåˆ¤å®š | STRONG_SHIFT, FACET_DEPENDENT, STABLE | âœ… _determine_cluster_type() | âœ… å®Œæˆ | - |
| ADD_NODEæ“ä½œ | cost=1.0 | âœ… _propose_add_node() | âœ… å®Œæˆ | - |
| **SPLIT_NODEæ“ä½œ** | **cost=2.0** | **âŒ TODOæ³¨é‡Š** | **âŒ ç¼ºå¤±** | **ğŸ”´ Critical** |
| **RENAME_NODEæ“ä½œ** | **cost=0.5** | **âŒ TODOæ³¨é‡Š** | **âŒ ç¼ºå¤±** | **ğŸ”´ Critical** |
| Objectiveé€‰æ‹© | FitGain - Î»Â·EditCost | âœ… Greedy selection | âœ… å®Œæˆ | - |
| **Reconstruction scores** | **å¯¹æ‰€æœ‰è§†è§’è®¡ç®—** | **âœ… compute_all_views_reconstruction()** | **âœ… å®Œæˆ** | - |

**å…³é”®ä»£ç ä½ç½®**:
- `knowledge/sns/modules/phase3_evolution.py`
- Lines 389-392: SPLIT/RENAMEçš„TODOæ³¨é‡Š
- Lines 419-512: **é‡è¦**: `compute_all_views_reconstruction()` å·²å®ç° (æ”¯æŒæ–°è®¾è®¡)

**éœ€è¦åšçš„ä¿®æ”¹**:
1. æ·»åŠ `_propose_split_node()` æ–¹æ³•åˆ°`EvolutionPlanner`
2. æ·»åŠ `_propose_rename_node()` æ–¹æ³•åˆ°`EvolutionPlanner`
3. åœ¨`plan_evolution()`ä¸­è°ƒç”¨è¿™ä¸¤ä¸ªæ–¹æ³•

### Phase 4: Delta-aware Guidance

**å®ç°åº¦**: 90%

| åŠŸèƒ½ | æ–¹æ³•è¦æ±‚ | ä»£ç å®ç° | çŠ¶æ€ | ä¼˜å…ˆçº§ |
|-----|---------|---------|------|--------|
| Reconstruct-then-select | å…ˆé‡æ„å†é€‰æ‹©ä¸»è½´ | âœ… select_main_axis_with_mode() | âœ… å®Œæˆ | - |
| Writing Modeåˆ¤å®š | EditCost>3.0 or FitGain>10.0 â†’ DELTA_FIRST | âœ… é˜ˆå€¼è§„åˆ™ | âœ… å®Œæˆ | - |
| Aux axisé€‰æ‹© | discriminativeness (variance) | âœ… select_aux_axis() | âœ… å®Œæˆ | - |
| Writing rulesç”Ÿæˆ | Mode-specific do/dont | âœ… _generate_writing_rules() | âœ… å®Œæˆ | - |
| Outlineç”Ÿæˆ | Cross-organize with main/aux | âœ… _generate_outline() | âœ… å®Œæˆ | - |
| **Taxonomy_v2** | **åº”ç”¨evolutionåˆ°tree** | **âŒ ä½¿ç”¨åŸå§‹tree** | **âŒ ç¼ºå¤±** | **ğŸ”´ Critical** |
| Must-answer questions | å…·ä½“é—®é¢˜ | âš ï¸ é€šç”¨é—®é¢˜ | âš ï¸ è´¨é‡ä½ | ğŸŸ¡ High |
| Evidence cards | ç²¾ç¡®quotes | âš ï¸ åªæœ‰abstract | âš ï¸ è´¨é‡ä½ | ğŸŸ¡ High |
| `guidance_pack.json` | æœºå™¨å¯è¯» | âœ… _save_guidance_pack() | âœ… å®Œæˆ | - |
| `audit_report.md` | äººç±»å¯è¯» | âœ… _generate_markdown_report() | âœ… å®Œæˆ | - |

**å…³é”®ä»£ç ä½ç½®**:
- `knowledge/sns/modules/phase4_guidance.py`
- `knowledge/sns/engine_v2.py` lines 442-606 (è¾“å‡ºç”Ÿæˆ)

**éœ€è¦åšçš„ä¿®æ”¹**:
1. æ·»åŠ `apply_evolution_to_taxonomy()` å‡½æ•°
2. åœ¨`Phase4Pipeline.run()`å¼€å§‹æ—¶åº”ç”¨evolution
3. å¢å¼º`_generate_must_answer_questions()` ç”Ÿæˆevolution-specificé—®é¢˜
4. å¢å¼º`_create_subsection()` ä»fit_reportsæå–ç²¾ç¡®evidence

---

## 3. æ”¹è¿›å®æ–½è®¡åˆ’

### ä¼˜å…ˆçº§1: é›†æˆçœŸå®Embeddingså’ŒNLIåˆ°Phase 2 (1-2 days)

**ç›®æ ‡**: æå‡FITåˆ¤å®šå‡†ç¡®æ€§

**æ–‡ä»¶**: `knowledge/sns/modules/phase2_stress_test.py`

**ä¿®æ”¹ç‚¹**:

#### ä¿®æ”¹1: EmbeddingBasedRetrieveré›†æˆçœŸå®embeddings

```python
# åœ¨æ–‡ä»¶é¡¶éƒ¨import
from ..embeddings import create_embedding_model

class EmbeddingBasedRetriever:
    def __init__(self, embedding_model_name: str = "specter2"):
        # ä½¿ç”¨çœŸå®embeddingæ¨¡å‹
        self.embedder = create_embedding_model(
            model_type=embedding_model_name,
            device="cpu"  # æˆ– "cuda" if available
        )
    
    def _compute_similarity(self, text1: str, text2: str) -> float:
        # ä½¿ç”¨çœŸå®embeddingè®¡ç®—
        emb1 = self.embedder.encode([text1])[0]
        emb2 = self.embedder.encode([text2])[0]
        return self.embedder.similarity(emb1, emb2)
```

#### ä¿®æ”¹2: FitTesteré›†æˆNLI

```python
# åœ¨æ–‡ä»¶é¡¶éƒ¨import
from ..nli import create_nli_model, compute_max_conflict_score

class FitTester:
    def __init__(self, retriever: EmbeddingBasedRetriever, nli_model=None):
        self.retriever = retriever
        # åˆå§‹åŒ–NLIæ¨¡å‹ (å¯é€‰,å¦‚æœä¸æä¾›åˆ™fallback)
        if nli_model is None:
            try:
                self.nli_model = create_nli_model(model_type="deberta", device="cpu")
            except Exception as e:
                logger.warning(f"Failed to load NLI model: {e}. Using fallback.")
                self.nli_model = create_nli_model(model_type="fallback")
        else:
            self.nli_model = nli_model
    
    def _calculate_conflict(self, claims: PaperClaims, node_def: NodeDefinition) -> float:
        # ä½¿ç”¨çœŸå®NLIè®¡ç®—
        all_claims_text = " ".join([
            c.text for claim_list in [claims.core_idea, claims.mechanism, claims.novelty_bullets]
            for c in claim_list
        ])
        
        return compute_max_conflict_score(
            claim_text=all_claims_text,
            node_definition_text=node_def.definition,
            exclusion_criteria=node_def.exclusion_criteria + node_def.boundary_statements,
            nli_model=self.nli_model
        )
```

#### ä¿®æ”¹3: æ›´æ–°Phase2Pipelineåˆå§‹åŒ–

```python
class Phase2Pipeline:
    def __init__(self, lm, embedding_model: str = "specter2", nli_model_type: str = "deberta"):
        self.claim_extractor = PaperClaimExtractor(lm)
        self.retriever = EmbeddingBasedRetriever(embedding_model)  # çœŸå®æ¨¡å‹
        
        # åˆå§‹åŒ–NLI
        try:
            nli_model = create_nli_model(model_type=nli_model_type, device="cpu")
        except Exception:
            logger.warning("Using fallback NLI")
            nli_model = None
        
        self.stress_tester = MultiViewStressTester(self.retriever, nli_model)
```

**é¢„æœŸæ•ˆæœ**:
- Coverageåˆ†æ•°å‡†ç¡® (åŸºäºSPECTER2)
- Conflictåˆ†æ•°å‡†ç¡® (åŸºäºDeBERTa-MNLI)
- FIT/FORCE_FIT/UNFITTABLEåˆ¤å®šæ›´å¯é 

---

### ä¼˜å…ˆçº§2: å®ç°è¡¥è§†è§’ç­–ç•¥ (2-3 days)

**ç›®æ ‡**: å½“baselineè´¨é‡ä¸è¶³æ—¶è‡ªåŠ¨è¡¥æ•‘

**æ–‡ä»¶**: æ–°å»º `knowledge/sns/modules/compensatory_view.py`

**æ ¸å¿ƒé€»è¾‘**:

```python
from collections import Counter
import numpy as np
from typing import List, Optional
import logging

try:
    import hdbscan
    HDBSCAN_AVAILABLE = True
except ImportError:
    HDBSCAN_AVAILABLE = False

from ..dataclass_v2 import (
    MultiViewBaseline, TaxonomyView, TaxonomyTree, TaxonomyTreeNode,
    NodeDefinition, EvidenceSpan, FacetLabel
)
from ...interface import Information

logger = logging.getLogger(__name__)


class CompensatoryViewInducer:
    """
    å½“baselineè´¨é‡ä¸è¶³æ—¶,ä»å‰æ²¿è®ºæ–‡è¯±å¯¼è¡¥è§†è§’ã€‚
    
    ç­–ç•¥:
    1. å¯¹å‰æ²¿è®ºæ–‡èšç±» (åŸºäºtitle+abstract embeddings)
    2. ä¸ºæ¯ä¸ªç°‡ç”Ÿæˆä¸»é¢˜æ ‡ç­¾ (ä½¿ç”¨LLM)
    3. æ„å»ºinduced taxonomy tree
    4. åˆ†é…æ–°çš„facet_label (é¿å…ä¸ç°æœ‰facetå†²çª)
    """
    
    def __init__(self, embedder, lm, min_cluster_size: int = 3):
        self.embedder = embedder
        self.lm = lm
        self.min_cluster_size = min_cluster_size
    
    def should_induce(self, baseline: MultiViewBaseline, min_facet_count: int = 2) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦è¡¥è§†è§’"""
        facet_counts = Counter([v.facet_label for v in baseline.views])
        
        # æ¡ä»¶1: unique facets < min_facet_count
        if len(facet_counts) < min_facet_count:
            return True
        
        # æ¡ä»¶2: dominant facet > 60%
        if baseline.views:
            for facet, count in facet_counts.items():
                if count / len(baseline.views) > 0.6:
                    return True
        
        return False
    
    def induce_view(
        self,
        baseline: MultiViewBaseline,
        papers: List[Information],
        topic: str
    ) -> Optional[TaxonomyView]:
        """è¯±å¯¼ä¸€ä¸ªè¡¥è§†è§’"""
        
        if not papers:
            logger.warning("No papers provided for compensatory view induction")
            return None
        
        logger.info(f"Inducing compensatory view from {len(papers)} papers...")
        
        # 1. å¯¹è®ºæ–‡èšç±»
        clusters = self._cluster_papers(papers)
        
        if not clusters:
            logger.warning("No clusters found, cannot induce view")
            return None
        
        # 2. ä¸ºæ¯ä¸ªç°‡ç”Ÿæˆæ ‡ç­¾
        cluster_labels = self._generate_cluster_labels(clusters)
        
        # 3. æ„å»ºinduced tree
        induced_tree = self._build_induced_tree(cluster_labels)
        
        # 4. é€‰æ‹©unique facet
        used_facets = {v.facet_label for v in baseline.views}
        new_facet = self._select_unique_facet(used_facets)
        
        # 5. åˆ›å»ºview
        view_id = f"T_induced_{len(baseline.views) + 1}"
        
        compensatory_view = TaxonomyView(
            view_id=view_id,
            review_id="INDUCED_FROM_PAPERS",
            review_title=f"Induced View: {new_facet.value} (from {len(papers)} papers)",
            facet_label=new_facet,
            facet_rationale=(
                f"Compensatory view induced from paper clustering to ensure baseline quality. "
                f"Represents emerging {new_facet.value.lower()} dimension."
            ),
            tree=induced_tree,
            node_definitions={},  # Will be populated
            weight=0.5,  # Slightly lower than normal reviews
            evidence=[]
        )
        
        # 6. æ„å»ºèŠ‚ç‚¹å®šä¹‰
        compensatory_view.node_definitions = self._build_node_definitions(
            induced_tree, clusters, cluster_labels
        )
        
        logger.info(f"Successfully induced view {view_id} with {len(cluster_labels)} categories")
        
        return compensatory_view
    
    def _cluster_papers(self, papers: List[Information]) -> List[List[Information]]:
        """ä½¿ç”¨HDBSCANå¯¹è®ºæ–‡èšç±»"""
        # æå–text
        texts = [f"{p.title} {p.description}" for p in papers]
        
        # ç”Ÿæˆembeddings
        embeddings = self.embedder.encode(texts)
        
        if not HDBSCAN_AVAILABLE:
            # Fallback: simple k-means or just split by keywords
            logger.warning("HDBSCAN not available, using simple fallback clustering")
            # ç®€å•åˆ†ä¸º3ç»„
            n_per_cluster = len(papers) // 3 + 1
            return [papers[i:i+n_per_cluster] for i in range(0, len(papers), n_per_cluster)]
        
        # HDBSCANèšç±»
        clusterer = hdbscan.HDBSCAN(
            min_cluster_size=self.min_cluster_size,
            metric='euclidean',
            cluster_selection_method='eom'
        )
        
        labels = clusterer.fit_predict(embeddings)
        
        # åˆ†ç»„
        clusters = {}
        for paper, label in zip(papers, labels):
            if label != -1:  # å¿½ç•¥å™ªå£°
                if label not in clusters:
                    clusters[label] = []
                clusters[label].append(paper)
        
        return list(clusters.values())
    
    def _generate_cluster_labels(self, clusters: List[List[Information]]) -> List[str]:
        """ä½¿ç”¨LLMä¸ºæ¯ä¸ªç°‡ç”Ÿæˆæ ‡ç­¾"""
        labels = []
        
        for i, cluster in enumerate(clusters):
            # æ„å»ºclusteræ‘˜è¦
            titles = [p.title for p in cluster[:5]]  # å‰5ç¯‡
            summary = f"Cluster {i+1} papers:\n" + "\n".join(f"- {t}" for t in titles)
            
            # è°ƒç”¨LLMç”Ÿæˆæ ‡ç­¾ (ç®€åŒ–:ä½¿ç”¨å…³é”®è¯æå–)
            # å®é™…åº”è¯¥ç”¨LLM: "Generate a short category name for these papers"
            # è¿™é‡Œç”¨ç®€åŒ–ç‰ˆæœ¬
            from collections import Counter
            words = []
            for p in cluster:
                words.extend(p.title.lower().split())
            
            common_words = Counter(words).most_common(3)
            label = "_".join([w for w, _ in common_words if len(w) > 3])
            
            labels.append(label or f"Category_{i+1}")
        
        return labels
    
    def _build_induced_tree(self, cluster_labels: List[str]) -> TaxonomyTree:
        """æ„å»ºinduced taxonomy tree (flat: root + leaves)"""
        # åˆ›å»ºroot
        root = TaxonomyTreeNode(
            name="Induced_Root",
            path="Induced_Root",
            parent=None,
            children=[],
            is_leaf=False
        )
        
        tree = TaxonomyTree(root=root)
        
        # æ·»åŠ leafèŠ‚ç‚¹
        for label in cluster_labels:
            leaf_path = f"Induced_Root/{label}"
            leaf_node = TaxonomyTreeNode(
                name=label,
                path=leaf_path,
                parent="Induced_Root",
                children=[],
                is_leaf=True
            )
            tree.add_node(leaf_node)
        
        return tree
    
    def _select_unique_facet(self, used_facets: set) -> FacetLabel:
        """é€‰æ‹©æœªä½¿ç”¨çš„facet"""
        all_facets = list(FacetLabel)
        
        for facet in all_facets:
            if facet not in used_facets and facet != FacetLabel.OTHER:
                return facet
        
        # å¦‚æœéƒ½ç”¨äº†,è¿”å›OTHERæˆ–é‡ç”¨
        return FacetLabel.OTHER
    
    def _build_node_definitions(
        self,
        tree: TaxonomyTree,
        clusters: List[List[Information]],
        labels: List[str]
    ) -> dict:
        """ä¸ºinduced treeçš„æ¯ä¸ªèŠ‚ç‚¹æ„å»ºå®šä¹‰"""
        node_defs = {}
        
        for label, cluster in zip(labels, clusters):
            node_path = f"Induced_Root/{label}"
            
            # æå–å…³é”®è¯
            keywords = self._extract_keywords(cluster)
            
            # æ„å»ºå®šä¹‰
            definition = f"Papers related to {label.replace('_', ' ')}"
            
            # ç”Ÿæˆinclusion/exclusion criteria (simplified)
            inclusion = [
                f"Papers about {kw}" for kw in keywords[:3]
            ]
            exclusion = ["Papers outside this theme"]
            
            # Evidence spans (ä»å‰3ç¯‡è®ºæ–‡)
            evidence = []
            for p in cluster[:3]:
                evidence.append(EvidenceSpan(
                    claim=f"Representative paper: {p.title}",
                    page=0,
                    section="Induced",
                    char_start=0,
                    char_end=len(p.description) if p.description else 0,
                    quote=p.description[:200] if p.description else ""
                ))
            
            node_def = NodeDefinition(
                node_path=node_path,
                definition=definition,
                inclusion_criteria=inclusion,
                exclusion_criteria=exclusion,
                canonical_keywords=keywords,
                boundary_statements=[],
                evidence_spans=evidence
            )
            
            node_defs[node_path] = node_def
        
        return node_defs
    
    def _extract_keywords(self, cluster: List[Information]) -> List[str]:
        """ä»clusteræå–å…³é”®è¯"""
        from collections import Counter
        
        words = []
        for p in cluster:
            words.extend(p.title.lower().split())
        
        # è¿‡æ»¤åœç”¨è¯ (ç®€åŒ–ç‰ˆ)
        stop_words = {'the', 'a', 'an', 'in', 'on', 'at', 'for', 'to', 'of', 'and', 'or'}
        words = [w for w in words if w not in stop_words and len(w) > 3]
        
        common = Counter(words).most_common(10)
        
        return [w for w, _ in common]
```

**é›†æˆåˆ°Phase 1**:

```python
# åœ¨ phase1_multiview_baseline.py

def _check_baseline_quality(self, baseline: MultiViewBaseline) -> None:
    """... ç°æœ‰ä»£ç  ..."""
    
    # æ–°å¢: è¡¥è§†è§’è§¦å‘
    if self.compensatory_inducer:
        if self.compensatory_inducer.should_induce(baseline):
            logger.warning("Baseline quality insufficient, triggering compensatory view induction")
            
            # éœ€è¦å‰æ²¿è®ºæ–‡ä½œä¸ºè¾“å…¥
            # (è¿™éœ€è¦åœ¨Phase1Pipelineä¸­æå‰æ£€ç´¢ä¸€äº›å‰æ²¿è®ºæ–‡)
            # compensatory_view = self.compensatory_inducer.induce_view(
            #     baseline, self.cached_papers, self.topic
            # )
            # if compensatory_view:
            #     baseline.views.append(compensatory_view)
            #     # é‡æ–°å½’ä¸€åŒ–æƒé‡
            #     baseline.__post_init__()
```

**é¢„æœŸæ•ˆæœ**:
- baselineå§‹ç»ˆæ»¡è¶³è´¨é‡è¦æ±‚ (â‰¥2 unique facets, no dominant >60%)
- å½“ç»¼è¿°æ•°æ®ä¸è¶³æ—¶è‡ªåŠ¨è¡¥æ•‘

---

### ä¼˜å…ˆçº§3: å®ç°SPLIT_NODEå’ŒRENAME_NODE (2-3 days)

**ç›®æ ‡**: å®Œæ•´æ”¯æŒ3ç§evolutionæ“ä½œ

**æ–‡ä»¶**: `knowledge/sns/modules/phase3_evolution.py`

**æ·»åŠ æ–¹æ³•** (è¯¦è§åˆ†ææ–‡æ¡£Section 4.3):
1. `_propose_split_node()` - å¤„ç†overcrowdedèŠ‚ç‚¹
2. `_propose_rename_node()` - å¤„ç†semantic drift

**é›†æˆåˆ°`plan_evolution()`**:

```python
def plan_evolution(...):
    # ... ç°æœ‰ADD_NODEé€»è¾‘ ...
    
    # æ–°å¢: Try SPLIT_NODE
    split_op = self._propose_split_node(cluster, view, fit_vectors)
    if split_op:
        candidates.append(split_op)
    
    # æ–°å¢: Try RENAME_NODE
    rename_op = self._propose_rename_node(cluster, view, fit_vectors)
    if rename_op:
        candidates.append(rename_op)
    
    # ... ç°æœ‰selectioné€»è¾‘ ...
```

**é¢„æœŸæ•ˆæœ**:
- å¯ä»¥å¤„ç†èŠ‚ç‚¹è¿‡åº¦æ‹¥æŒ¤åœºæ™¯
- å¯ä»¥å¤„ç†è¯­ä¹‰æ¼‚ç§»åœºæ™¯
- Evolution proposalæ›´å…¨é¢

---

### ä¼˜å…ˆçº§4: åº”ç”¨Evolutionåˆ°Taxonomyç”Ÿæˆv2 (1-2 days)

**ç›®æ ‡**: ä¸‹æ¸¸ç³»ç»Ÿçœ‹åˆ°æ¼”åŒ–åçš„taxonomy

**æ–‡ä»¶**: æ–°å»º `knowledge/sns/modules/taxonomy_evolution_applier.py`

**æ ¸å¿ƒå‡½æ•°**:

```python
import copy
from ..dataclass_v2 import (
    TaxonomyView, EvolutionOperation, AddNodeOperation,
    SplitNodeOperation, RenameNodeOperation,
    TaxonomyTreeNode, NodeDefinition
)


def apply_evolution_to_taxonomy(
    view: TaxonomyView,
    operations: List[EvolutionOperation]
) -> TaxonomyView:
    """
    å°†accepted evolution operationsåº”ç”¨åˆ°taxonomy tree,ç”Ÿæˆtaxonomy_v2ã€‚
    """
    view_v2 = copy.deepcopy(view)
    
    for op in operations:
        if op.view_id != view.view_id:
            continue  # åªåº”ç”¨åˆ°å¯¹åº”view
        
        if isinstance(op, AddNodeOperation):
            _apply_add_node(view_v2, op)
        
        elif isinstance(op, SplitNodeOperation):
            _apply_split_node(view_v2, op)
        
        elif isinstance(op, RenameNodeOperation):
            _apply_rename_node(view_v2, op)
    
    return view_v2
```

**é›†æˆåˆ°Phase 4**:

```python
# åœ¨ phase4_guidance.py

class Phase4Pipeline:
    def run(self, ...):
        # ... ç°æœ‰ä»£ç  ...
        
        # **æ–°å¢: åœ¨axis selectionä¹‹å‰,åº”ç”¨evolution**
        from ..modules.taxonomy_evolution_applier import apply_evolution_to_taxonomy
        
        # ä¸ºæ‰€æœ‰è§†è§’åº”ç”¨evolution
        baseline_v2 = copy.deepcopy(baseline)
        for i, view in enumerate(baseline_v2.views):
            view_operations = [op for op in evolution_proposal.operations if op.view_id == view.view_id]
            if view_operations:
                baseline_v2.views[i] = apply_evolution_to_taxonomy(view, view_operations)
        
        # ä½¿ç”¨evolved baselineè¿›è¡Œaxis selection
        main_axis, main_axis_mode = self.axis_selector.select_main_axis_with_mode(
            reconstruction_scores,
            baseline_v2  # ä½¿ç”¨v2
        )
        
        # ...
```

**é¢„æœŸæ•ˆæœ**:
- `guidance_pack.json` ä¸­çš„taxonomyæ˜¯æ¼”åŒ–åçš„ç‰ˆæœ¬
- ä¸‹æ¸¸ç³»ç»Ÿå¯ä»¥çœ‹åˆ°ADD_NODE/SPLIT_NODE/RENAME_NODEçš„ç»“æœ

---

## 4. ä¿®æ”¹æ–‡ä»¶æ¸…å•

| æ–‡ä»¶ | ä¿®æ”¹ç±»å‹ | ä¼˜å…ˆçº§ | é¢„ä¼°å·¥ä½œé‡ |
|-----|---------|--------|-----------|
| `phase2_stress_test.py` | ä¿®æ”¹ (é›†æˆembeddings+NLI) | ğŸ”´ Critical | 4-6 hours |
| `compensatory_view.py` | æ–°å»º | ğŸ”´ Critical | 1-2 days |
| `phase1_multiview_baseline.py` | ä¿®æ”¹ (é›†æˆcompensatory) | ğŸ”´ Critical | 2-4 hours |
| `phase3_evolution.py` | æ·»åŠ SPLIT/RENAMEæ–¹æ³• | ğŸ”´ Critical | 1-2 days |
| `taxonomy_evolution_applier.py` | æ–°å»º | ğŸ”´ Critical | 1-2 days |
| `phase4_guidance.py` | ä¿®æ”¹ (åº”ç”¨evolution, å¢å¼ºquestions) | ğŸŸ¡ High | 4-8 hours |
| `engine_v2.py` | å°ä¿®æ”¹ (pipelineå‚æ•°) | ğŸŸ¢ Low | 1-2 hours |

**æ€»é¢„ä¼°**: 5-7ä¸ªå·¥ä½œæ—¥

---

## 5. æµ‹è¯•è®¡åˆ’

### å•å…ƒæµ‹è¯•

1. **Embeddingsé›†æˆæµ‹è¯•**:
   - æµ‹è¯•`EmbeddingBasedRetriever`ä½¿ç”¨çœŸå®SPECTER2
   - éªŒè¯ç›¸ä¼¼åº¦åˆ†æ•°èŒƒå›´ [0, 1]

2. **NLIé›†æˆæµ‹è¯•**:
   - æµ‹è¯•`FitTester`ä½¿ç”¨çœŸå®DeBERTa-MNLI
   - éªŒè¯å†²çªæ£€æµ‹å‡†ç¡®æ€§

3. **è¡¥è§†è§’æµ‹è¯•**:
   - æµ‹è¯•`CompensatoryViewInducer.should_induce()`
   - æµ‹è¯•è¯±å¯¼è§†è§’çš„treeç»“æ„

4. **Evolutionåº”ç”¨æµ‹è¯•**:
   - æµ‹è¯•`apply_evolution_to_taxonomy()`
   - éªŒè¯treeç»“æ„æ›´æ–°æ­£ç¡®

### é›†æˆæµ‹è¯•

1. **End-to-end Pipeline**:
   - è¿è¡Œå®Œæ•´Phase 1-4
   - éªŒè¯è¾“å‡ºæ–‡ä»¶æ ¼å¼
   - æ£€æŸ¥`guidance_pack.json` schema

2. **è´¨é‡éªŒè¯**:
   - æ£€æŸ¥FIT/FORCE_FIT/UNFITTABLEåˆ†å¸ƒåˆç†æ€§
   - éªŒè¯evolution operationsæœ‰è¯æ®æ”¯æŒ
   - éªŒè¯writing_rulesç¬¦åˆmode

---

## 6. éƒ¨ç½²å»ºè®®

### Phase 1: å†…éƒ¨æµ‹è¯• (Week 1-2)

1. å®Œæˆä¼˜å…ˆçº§1-2ä¿®æ”¹
2. åœ¨å°è§„æ¨¡æ•°æ®ä¸Šæµ‹è¯•
3. è°ƒæ•´é˜ˆå€¼å’Œå‚æ•°

### Phase 2: Betaæµ‹è¯• (Week 3-4)

1. å®Œæˆä¼˜å…ˆçº§3-4ä¿®æ”¹
2. åœ¨çœŸå®æ•°æ®ä¸Šæµ‹è¯•
3. æ”¶é›†ç”¨æˆ·åé¦ˆ

### Phase 3: ç”Ÿäº§éƒ¨ç½² (Week 5+)

1. æ€§èƒ½ä¼˜åŒ– (embeddingç¼“å­˜, batch processing)
2. æ–‡æ¡£è¡¥å……
3. å‘å¸ƒv2.0

---

## 7. æœ€ç»ˆè¯„ä¼°

### å½“å‰å®ç°å®Œæ•´åº¦

| Phase | å®Œæ•´åº¦ | å…³é”®ç¼ºå¤± |
|-------|--------|---------|
| Phase 1 | 85% | è¡¥è§†è§’ç­–ç•¥ |
| Phase 2 | 70% | çœŸå®Embeddings+NLI |
| Phase 3 | 75% | SPLIT/RENAMEæ“ä½œ |
| Phase 4 | 90% | Taxonomy_v2åº”ç”¨ |
| **æ€»ä½“** | **80%** | **4ä¸ªCritical issues** |

### æ–¹æ³•è¯´æ˜å¯¹é½åº¦

- **è®¾è®¡åŸåˆ™**: âœ… 100% å¯¹é½
- **æ•°æ®ç»“æ„**: âœ… 100% å¯¹é½
- **Pipelineæµç¨‹**: âœ… 95% å¯¹é½
- **åŠŸèƒ½å®Œæ•´æ€§**: âš ï¸ 80% å®Œæ•´

### ä»£ç è´¨é‡è¯„ä»·

- **æ¶æ„è®¾è®¡**: â­â­â­â­â­ ä¼˜ç§€
- **å¯æ‰©å±•æ€§**: â­â­â­â­â­ ä¼˜ç§€
- **æ–‡æ¡£è´¨é‡**: â­â­â­â­â˜† è‰¯å¥½
- **æµ‹è¯•è¦†ç›–**: â­â­â­â˜†â˜† ä¸­ç­‰
- **ç”Ÿäº§å°±ç»ª**: â­â­â­â˜†â˜† éœ€è¦è¡¥é½Critical issues

---

## 8. ç»“è®ºä¸å»ºè®®

### ç»“è®º

1. **å·²æœ‰åšå®åŸºç¡€**: ä»£ç æ¶æ„ä¼˜ç§€,æ ¸å¿ƒframeworkå®Œæ•´
2. **å…³é”®ç»„ä»¶å·²å®ç°**: Embeddingså’ŒNLIæ¨¡å—å·²å­˜åœ¨,åªéœ€é›†æˆ
3. **ä¸»è¦æ˜¯é›†æˆå·¥ä½œ**: å¤§éƒ¨åˆ†ç¼ºå¤±åŠŸèƒ½æ˜¯"è¿æ¥ç°æœ‰ç»„ä»¶"è€Œé"ä»é›¶å®ç°"
4. **å¯å¿«é€Ÿè¡¥é½**: é¢„è®¡5-7ä¸ªå·¥ä½œæ—¥å¯å®Œæˆæ‰€æœ‰Critical issues

### å»ºè®®ä¼˜å…ˆé¡ºåº

1. **Week 1**: é›†æˆçœŸå®Embeddings+NLIåˆ°Phase 2 (æœ€å¤§ROI)
2. **Week 2**: å®ç°è¡¥è§†è§’ç­–ç•¥ (ä¿è¯baselineè´¨é‡)
3. **Week 3**: å®ç°SPLIT/RENAME + åº”ç”¨Evolution
4. **Week 4**: è´¨é‡æå‡ (questions, evidence cards, æµ‹è¯•)

### é£é™©ä¸ç¼“è§£

| é£é™© | æ¦‚ç‡ | å½±å“ | ç¼“è§£æªæ–½ |
|-----|-----|-----|---------|
| Embeddingæ¨¡å‹åŠ è½½å¤±è´¥ | ä¸­ | é«˜ | Fallbackæœºåˆ¶å·²å®ç° |
| NLIæ¨¡å‹å¤ªæ…¢ | ä¸­ | ä¸­ | Batch processing, ç¼“å­˜ |
| è¡¥è§†è§’è´¨é‡å·® | ä½ | ä¸­ | LLMç”Ÿæˆlabel, äººå·¥review |
| Evolutionåº”ç”¨bug | ä½ | é«˜ | å……åˆ†æµ‹è¯•, tree validation |

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**æœ€åæ›´æ–°**: 2025-12-15  
**ä½œè€…**: Claude (AI Code Assistant)
